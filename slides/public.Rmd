---
title: "A fast sampler for data simulation from spatial, and other, Markov random fields"
shorttitle: "Conclique-based Gibbs"
author: Andee Kaplan
shortname: Kaplan, et al.
institute: |
    | Iowa State University
    | ajkaplan@iastate.edu
shortinstitute: ajkaplan@iastate.edu
date: |
  | June 22, 2017
  |
  | Slides available at <http://bit.ly/kaplan-phd>
  |
  | \footnotesize Joint work with M. Kaiser, S. Lahiri, and D. Nordman
shortdate: "June 22, 2017"
output: 
  beamer_presentation:
    keep_tex: false
    template: beamer.tex
    includes:
      in_header: front-matter.tex
theme: CambridgeUS
bibliography: [refs.bib, ../resources/refs_conclique.bib]
fig_caption: true
nocite: |
    @kaiser2007statistical, @hammersley1971markov, @besag1974spatial, @besag1999bayesian
---

```{r libraries, echo=FALSE, message=FALSE, warning=FALSE, cache=FALSE}
library(knitr)
library(ggplot2)
library(dplyr)
library(tidyr)
library(conclique)
library(mvtnorm)
library(rootSolve)
library(agridat)
library(xtable)


set.seed(1022)
opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE)
theme_set(theme_bw(base_family = "serif", base_size = 20))
```

# Overview

**Thesis:** On advancing MCMC-based methods for Markovian data structures with applications to deep learning, simulation, and resampling

**Goal:** Develop statistical inference via Markov chain Monte Carlo (MCMC) techniques in complex data problems related to statistical learning, the analysis of network/graph data, and spatial resampling

**Challenge:** Develop model-based methodology, which is both *statistically rigorous* and *computationally scalable*, by exploiting conditional independence 

1. Statistical properties of graph models used in deep machine learning and image classification   
    (Ch. 2 & 3)
2. Fast methods for simulating spatial, network, and other data  
    (Ch. 4 & 5)


# Goal

- Markov random field models are popular for spatial or network data
\vspace{.2in}
- Rather than specifying a joint distribution directly, a  model is specified through a set of full conditional distributions for each spatial location
\vspace{.2in}
- Conditional distributions are assumed to correspond to a valid joint (e.g., sufficient conditions in @kaiser2000construction)

\vspace{.4in}
**Goal:** A new, provably fast approach for simulating spatial/network data



# Spatial Markov random field (MRF) models

\begin{block}{Notation}
\begin{itemize}
\itemsep .1in
\item Variables  $\{ Y(\mbs_i): i=1, \dots, n \}$ at locations $\{ \mbs_i: i=1, \dots, n\}$
\item Neighborhoods: $\mathcal{N}_i$ specified according to some configuration
\item Neighboring Values: $\mby(\mathcal{N}_i) = \{ y(\mbs_j) : \mbs_j \in \mathcal{N}_i \}$
\item Full Conditionals: $\{ f_i(y(\mbs_i)|\mby(\mathcal{N}_i), \mbtheta): i=1, \dots, n \}$
\vspace{.1in}
\begin{itemize}
  \itemsep .2in
    \item $f_i(y(\mbs_i)|\mby(\mathcal{N}_i), \mbtheta)$ is conditional pmf/pdf of $Y(\mbs_i)$ given values for its neighbors  $\mby(\mathcal{N}_i)$
    \item Often assume a common conditional cdf $F_i=F$ form ($f_i=f$) for all $i$
\end{itemize}
\end{itemize}
\end{block}

Formulation adaptable to non-spatial data by letting $\boldsymbol s_i$ denote a marker for observation $Y(\boldsymbol s_i)$ (e.g., random graphs, $\boldsymbol s_i$ represents a potential edge and $Y(\boldsymbol s)\in\{0,1\}$)   

# Common neighborhood structures


\begin{columns}[T] % align columns
\begin{column}{.49\textwidth}
{\bf 4-nearest neighborhood}  

Defined by locations in cardinal directions 

$$
\begin{array}{ccc}
\cdot&*&\cdot\\
*&\boldsymbol s_i&*\\
\cdot&*&\cdot\\
\end{array}
$$

$$
\mathcal{N}_i = \{\boldsymbol s_i \pm (0, 1)\} \bigcup \{\boldsymbol s_i \pm (1, 0)\}
$$
\end{column}
\begin{column}{.49\textwidth}
{\bf 8-nearest neighborhood}  

Also includes neighboring diagonals 
\vspace{.2in}

$$
\begin{array}{ccc}
*&*&*\\
*&\boldsymbol s_i&*\\
*&*&*\\
\end{array}
$$

\begin{align*}
\mathcal{N}_i &= \{\boldsymbol s_i \pm (0, 1)\} \bigcup \{\boldsymbol s_i \pm (1, 0)\} \bigcup \\
&\qquad \{\boldsymbol s_i \pm (1, -1)\} \bigcup \{\boldsymbol s_i \pm (1, 1)\}
\end{align*}
\end{column}
\end{columns}


# Exponential family examples

1. Conditional Gaussian (3 parameters):  
    $$
      f_i(y(\mbs_i)|\mby(\mathcal{N}_i),\alpha,\eta,\tau) = \frac{1}{\sqrt{2 \pi} \tau}\exp\left( -\frac{[y(\mbs_i) - \mu(\mbs_i) ]^2}{2 \tau^2}\right)
    $$
    $Y(\mbs_i)$ given neighbors $\mby(\mathcal{N}_i)$ is normal with variance $\tau^2$ and mean
    $$
    \mu(\mbs_i) = \alpha + \eta \sum_{\mbs_j \in \mathcal{N}_i}[y(\mbs_j)-\alpha]
    $$
2. Conditional Binary (2 parameters):  
    $Y(\mbs_i)$ given neighbors $\mby(\mathcal{N}_i)$ is Bernoulli $p(\mbs_i,\kappa,\eta)$ where
    $$
    \mathrm{logit}[p(\mbs_i,\kappa,\eta)] = \mathrm{logit}( \kappa ) +\eta \sum_{\mbs_j \in \mathcal{N}_i}[y(\mbs_j)-\kappa]
    $$

In both examples, $\eta$ represents a dependence parameter.


# Illustrative Example

```{r endive-data}
data(besag.endive)
endive <- besag.endive
m <- max(endive$row)
n <- max(endive$col)
```

- For context, illustrate some common simulation demands arising in inference about spatial Markov models 
- Spatial dataset from @besag1977some 
- Binary observations located on a $14\times 179$ indicating the presence or absence of footrot in endive plants

```{r endive-data-plot, fig.height=3.5}
ggplot(endive, aes(col, row, fill = disease)) +
  geom_tile() +
  scale_fill_manual("Disease present", values = c("grey80", "black")) +
  xlab("Column") + ylab("Row") +
  theme(text = element_text(size=20))

# encode
endive$disease <- ifelse(endive$disease == "Y", 1, 0)
```

# Three spatial binary models

1. Isotropic centered autologistic model [@caragea2009autologistic; @besag1972nearest; @besag1977some]
\vspace{.25in}
2. Centered autologistic model with two dependence parameters 
\vspace{.25in}
3. Centered autologistic model as in (2) but having large scale structure determined by regression on the horizontal coordinate $u_i$ of each spatial location $\boldsymbol s_i=(u_i,v_i)$.

# Three models (Cont'd)

Conditional mass function of the form
$$
f_i(y(\boldsymbol s_i)| \boldsymbol y(\mathcal{N}_i), \boldsymbol \theta) =\frac{\exp[y(\boldsymbol s_i) A_i\{\boldsymbol y(\mathcal{N}_i)\}]}{ 1 +\exp[y(\boldsymbol s_i) A_i\{\boldsymbol y(\mathcal{N}_i)\}}, \quad y(\boldsymbol s_i)=0,1, 
$$ 
with
\begin{table}[ht!]
\centering
\footnotesize
\begin{tabular}{| l | p{4in} |}
\hline
Model &  Natural parameter function\\
\hline
(1) & $A_i\{\boldsymbol y(\mathcal{N}_i)\} = \log\left(\frac{\kappa}{1-\kappa}\right) + \eta\sum\limits_{\boldsymbol s_j \in \mathcal{N}_i}\{y(\boldsymbol s_j) - \kappa\}$ \\
(2) & $A_i\{\boldsymbol y(\mathcal{N}_i)\} = \log\left(\frac{\kappa}{1-\kappa}\right) + \eta_u\sum\limits_{\boldsymbol s_j \in N_{u,i}}\{y(\boldsymbol s_j) - \kappa\} + \eta_v\sum\limits_{\boldsymbol s_j \in N_{v,i}}\{y(\boldsymbol s_j) - \kappa\}$ \\
(3) &  {\vspace{-.175in} \begin{align*} &A_i\{\boldsymbol y(\mathcal{N}_i)\} = \log\left(\frac{\kappa_i}{1-\kappa_i}\right) + \eta_u\sum\limits_{\boldsymbol s_j \in N_{u,i}}\{y(\boldsymbol s_j) - \kappa_i\} + \eta_v\sum\limits_{\boldsymbol s_j \in N_{v,i}}\{y(\boldsymbol s_j) - \kappa_i\}, \\
&\log\left(\frac{\kappa_i}{1-\kappa_i}\right) = \beta_0 + \beta_1 u_i \end{align*}}\\
\hline
\end{tabular}
\end{table}

# Bootstrap percentile confidence intervals

- Fit three models of increasing complexity to these data via pseudo-likelihood [@besag1975statistical]  
- Apply simulation (parametric bootstrap) to obtain reference distributions for statistics based on the resulting estimators
- This involves the Gibbs sampler (due to the conditional model specification) & where computational demands arise

```{r endive-model-setup}
#create concliques list -------------------------------
concliques_4nn <- function(grid) {
  concliques <- list()
  concliques[[1]] <- grid[(row(grid) %% 2 == 1 & col(grid) %% 2 == 1) | (row(grid) %% 2 == 0 & col(grid) %% 2 == 0)]
  concliques[[2]] <- grid[(row(grid) %% 2 == 0 & col(grid) %% 2 == 1) | (row(grid) %% 2 == 1 & col(grid) %% 2 == 0)]
  class(concliques) <- "conclique_cover"
  return(concliques)
}

#create grid, lattices, neighbors, concliques for MCMC ------------------
grid <- matrix(1:(m*n), nrow = m)
concliques <- concliques_4nn(grid)
lattice <- lattice_4nn_torus(dimvec = c(m, n))
neighbors_one_param <- get_neighbors(lattice)
neighbors_two_param <- get_neighbors(lattice, TRUE, grid)
inits <- matrix(rbinom(m*n, 1, .5), nrow = m)
```
```{r binary-functs}
#custom functions -----------

#pseudo likelihood fit for conditonal binary model (4 neighbors)
fit_one_param <- function(data, neighbors, ...) {
  ## sum of neighbors
  neigh_sum <- rowSums(matrix(data[neighbors[[1]][,-1]], nrow = length(data)))
  
  ## fit logistic conditional model: the conditonal prob p of Y given 4 neighbor sum S
  ## is logit(p) = logit(kappa) + eta*(S-4*kappa) or logit(p) = int + eta*S for int = logit(kappa)-4*eta*kappa
  model <- glm(data ~ neigh_sum, family = binomial())
  intercept <- as.numeric(model$coefficients[1])
  eta <- as.numeric(model$coefficients[2])
  
  ## retrieve kappa from intercept and eta by finding root of g on (0,1)
  g <- function(y, params) {
    eta <- params$eta
    intercept <- params$intercept
    
    return(log(y) - log(1 - y) - 4*y*eta - intercept)
  }
  kappa <- uniroot(g, c(0, 1), list(eta = eta, intercept = intercept))$root
  
  list(eta = eta, kappa = kappa)
}
fit_two_param <- function(data, neighbors, ...) {
  ## sum of neighbors
  neigh_sum <- do.call(cbind, lapply(neighbors, function(x) { 
    rowSums(matrix(data[x[,-1]], nrow = length(data))) 
    }))
  
  ## fit logistic conditional model: the conditonal prob p of Y given 4 neighbor sum S
  ## is logit(p) = logit(kappa) + eta1*(S1-2*kappa) + eta2*(S2-2*kappa) or 
  ## logit(p) = int + eta1*S1 + eta2*S2 for int = logit(kappa)-2*eta1*kappa - 2*eta2*kappa
  model <- glm(data ~ neigh_sum, family = binomial())
  intercept <- as.numeric(model$coefficients[1])
  eta_1 <- as.numeric(model$coefficients[2])
  eta_2 <- as.numeric(model$coefficients[3])
  
  ## retrieve kappa from intercept and eta by finding root of g on (0,1)
  g <- function(y, params) {
    eta_1 <- params$eta_1
    eta_2 <- params$eta_2
    intercept <- params$intercept
    
    return(log(y) - log(1 - y) - 2*y*eta_1 - 2*y*eta_2 - intercept)
  }
  kappa <- uniroot(g, c(0, 1), list(eta_1 = eta_1, eta_2 = eta_2, intercept = intercept))$root
  
  list(eta_1 = eta_1, eta_2 = eta_2, kappa = kappa)
}
fit_two_param_reg <- function(data, neighbors, params0, cols, ...) {
  res <- list()
  res$data <- data
  res$u <- (0:(length(res$data) - 1)) %% cols + 1
  res$nums <- lapply(neighbors, function(neigh) {
    rowSums(!is.na(neigh[, -1]))
  }) 
  res$sums <- lapply(neighbors, function(neigh) {
    rowSums(matrix(res$data[neigh[, -1]], ncol = ncol(neigh) - 1, byrow = TRUE))
  }) 
  
  
  #pseudo likelihood functions
  logf <- function(params, data, fixed) {
    u <- data$u
    beta <- params
    eta <- fixed
    
    reg <- beta[1] + beta[2]*u
    kappa <- exp(reg)/(1 + exp(reg))
    A <- reg + eta[1]*(data$sums[[1]] - data$nums[[1]]*kappa) + eta[2]*(data$sums[[2]] - data$nums[[2]]*kappa)
    B <- log(1 + exp(A))
    
    sum(A*data$data - B)
  }
  grad <- function(params, data, fixed) {
    u <- data$u
    beta <- params
    eta <- fixed
    
    reg <- beta[1] + beta[2]*u
    kappa <- exp(reg)/(1 + exp(reg))
    A <- reg + eta[1]*(data$sums[[1]] - data$nums[[1]]*kappa) + eta[2]*(data$sums[[2]] - data$nums[[2]]*kappa)
    
    dk_db0 <- exp(reg)/(1 + exp(reg))^2
    dk_db1 <- u*dk_db0
    dA_dk <- 1/kappa + 1/(1 - kappa) - 2*(eta[1] + eta[2])
    dlogf_dA <- data$data
    dlogf_dB <- -1
    dB_dA <- exp(A)/(1 + exp(A))
    
    dlogf_b0 <- sum(dlogf_dA*dA_dk*dk_db0 + dlogf_dB*dB_dA*dA_dk*dk_db0)
    dlogf_b1 <- sum(dlogf_dA*dA_dk*dk_db1 + dlogf_dB*dB_dA*dA_dk*dk_db1)
    
    c(dlogf_b0, dlogf_b1)
  }
  
  beta0 <- c(params0$beta_0, params0$beta_1)
  
  ## fit logistic conditional model: the conditonal prob p of Y given 4 neighbor sum S
  ## is logit(p) = logit(kappa) + eta1*(S1-2*kappa) + eta2*(S2-2*kappa) or 
  ## logit(p) = int + eta1*S1 + eta2*S2 for int = logit(kappa)-2*eta1*kappa - 2*eta2*kappa
  sums <- do.call(cbind, res$sums)
  model <- glm(res$data ~ sums, family = binomial())
  eta <- as.numeric(model$coefficients[-1])  
  
  ## pseudo likelihood for eta
  plik <- optim(beta0, logf, gr = grad, data = res, fixed = eta, control = list(fnscale = -1))
  beta <- as.numeric(plik$par)
  
  params <- list(beta_0 = beta[1], beta_1 = beta[2], eta_1 = eta[1], eta_2 = eta[2])
  return(params)
}

#cdfs 
cdf_one_param <- function(data, params) {
  eta <- params$eta
  kappa <- params$kappa
  
  mean_structure <- log(kappa) - log(1 - kappa) + eta*(data$sums[[1]] - data$nums[[1]]*kappa)
  p <- exp(mean_structure)/(1 + exp(mean_structure))
  vapply(seq_along(mean_structure), FUN = function(i) pbinom(data$data[i], 1, p[i]), FUN.VALUE = numeric(1))
}
cdf_two_param <- function(data, params) {
  eta_1 <- params$eta_1
  eta_2 <- params$eta_2
  kappa <- params$kappa
  
  mean_structure <- log(kappa) - log(1 - kappa) + eta_1*(data$sums[[1]] - data$nums[[1]]*kappa) + eta_2*(data$sums[[2]] - data$nums[[2]]*kappa)
  p <- exp(mean_structure)/(1 + exp(mean_structure))
  vapply(seq_along(mean_structure), FUN = function(i) pbinom(data$data[i], 1, p[i]), FUN.VALUE = numeric(1))
}
cdf_two_param_reg <- function(data, params) {
  eta_1 <- params$eta_1
  eta_2 <- params$eta_2
  beta_0 <- params$beta_0
  beta_1 <- params$beta_1
  
  u <- data$u
  reg <- beta_0 + beta_1*u
  kappa <- exp(reg)/(1 + exp(reg))
  
  mean_structure <- reg + eta_1*(data$sums[[1]] - data$nums[[1]]*kappa) + eta_2*(data$sums[[2]] - data$nums[[2]]*kappa)
  p <- exp(mean_structure)/(1 + exp(mean_structure))
  vapply(seq_along(mean_structure), FUN = function(i) pbinom(data$data[i], 1, p[i]), FUN.VALUE = numeric(1))
}

#pmfs
pmf_one_param <- function(data, params) {
  eta <- params$eta
  kappa <- params$kappa
  
  mean_structure <- log(kappa) - log(1 - kappa) + eta*(data$sums[[1]] - data$nums[[1]]*kappa)
  p <- exp(mean_structure)/(1 + exp(mean_structure))
  vapply(seq_along(mean_structure), FUN = function(i) dbinom(data$data[i], 1, p[i]), FUN.VALUE = numeric(1))
}
pmf_two_param <- function(data, params) {
  eta_1 <- params$eta_1
  eta_2 <- params$eta_2
  kappa <- params$kappa
  
  mean_structure <- log(kappa) - log(1 - kappa) + eta_1*(data$sums[[1]] - data$nums[[1]]*kappa) + eta_2*(data$sums[[2]] - data$nums[[2]]*kappa)
  p <- exp(mean_structure)/(1 + exp(mean_structure))
  vapply(seq_along(mean_structure), FUN = function(i) dbinom(data$data[i], 1, p[i]), FUN.VALUE = numeric(1))
}
pmf_two_param_reg <- function(data, params) {
  eta_1 <- params$eta_1
  eta_2 <- params$eta_2
  beta_0 <- params$beta_0
  beta_1 <- params$beta_1
  
  u <- data$u
  reg <- beta_0 + beta_1*u
  kappa <- exp(reg)/(1 + exp(reg))
  
  mean_structure <- reg + eta_1*(data$sums[[1]] - data$nums[[1]]*kappa) + eta_2*(data$sums[[2]] - data$nums[[2]]*kappa)
  p <- exp(mean_structure)/(1 + exp(mean_structure))
  vapply(seq_along(mean_structure), FUN = function(i) dbinom(data$data[i], 1, p[i]), FUN.VALUE = numeric(1))
}

#samplers
sampler_two_param_reg <- function(data, params) {
  eta_1 <- params$eta_1
  eta_2 <- params$eta_2
  beta_0 <- params$beta_0
  beta_1 <- params$beta_1
  
  u <- data$u
  reg <- beta_0 + beta_1*u
  kappa <- exp(reg)/(1 + exp(reg))
  
  mean_structure <- reg + eta_1*(data$sums[[1]] - data$nums[[1]]*kappa) + eta_2*(data$sums[[2]] - data$nums[[2]]*kappa)
  vapply(exp(mean_structure)/(1 + exp(mean_structure)), FUN = function(p) rbinom(1, 1, p), FUN.VALUE = numeric(1))
}
```
```{r bootstrap-setup}
B <- 10000
burnin <- 1000
thin <- 5
iter <- B*thin + burnin
```
```{r endive-boot-one}
params_est_one_param <- fit_one_param(endive$disease, neighbors_one_param)

y_star_one_param <- run_conclique_gibbs(concliques, neighbors_one_param, inits, "binary_single_param", params_est_one_param, iter)
y_star_one_param <- y_star_one_param[(burnin:iter)[burnin:iter %% thin == 1], ]
gof_stat_star_one_param <- rep(NA, B)
params_star_one_param <- data.frame(eta = rep(NA, B), kappa = rep(NA, B))

for(i in 1:B) {
  dat_one_param <- y_star_one_param[i,]
  params_star_one <- fit_one_param(dat_one_param, neighbors_one_param)
  params_star_one_param[i, ] <- c(params_star_one$eta, params_star_one$kappa)
  resids_star_one_param <- spatial_residuals(dat_one_param, neighbors_one_param, "cdf_one_param", params_star_one, discrete = "pmf_one_param")
  gof_stat_star_one_param[i] <- gof_statistics(resids_star_one_param, concliques, "ks", "max")
}

resids_one_param <- spatial_residuals(endive$disease, neighbors_one_param, "cdf_one_param", params_est_one_param, discrete = "pmf_one_param")
gof_stat_one_param <- gof_statistics(resids_one_param, concliques, "ks", "max")
p_value_one_param <- (sum(gof_stat_star_one_param >= gof_stat_one_param) + 1)/(B + 1)
```
```{r endive-boot-two}

params_est_two_param <- fit_two_param(endive$disease, neighbors_two_param)

y_star_two_param <- run_conclique_gibbs(concliques, neighbors_two_param, inits, "binary_two_param", params_est_two_param, iter)
y_star_two_param <- y_star_two_param[(burnin:iter)[burnin:iter %% thin == 1], ]
gof_stat_star_two_param <- rep(NA, B)
params_star_two_param <- data.frame(eta_1 = rep(NA, B), eta_2 = rep(NA, B), kappa = rep(NA, B))

for(i in 1:B) {
  dat_two_param <- y_star_two_param[i,]
  params_star_two <- fit_two_param(dat_two_param, neighbors_two_param)
  params_star_two_param[i, ] <- c(params_star_two$eta_1, params_star_two$eta_2, params_star_two$kappa)
  resids_star_two_param <- spatial_residuals(dat_two_param, neighbors_two_param, "cdf_two_param", params_star_two, discrete = "pmf_two_param")
  gof_stat_star_two_param[i] <- gof_statistics(resids_star_two_param, concliques, "ks", "max")
}

resids_two_param <- spatial_residuals(endive$disease, neighbors_two_param, "cdf_two_param", params_est_two_param, discrete = "pmf_two_param")
gof_stat_two_param <- gof_statistics(resids_two_param, concliques, "ks", "max")
p_value_two_param <- (sum(gof_stat_star_two_param >= gof_stat_two_param) + 1)/(B + 1)

```
```{r endive-boot-two-reg}
params_est_two_param_reg <- fit_two_param_reg(endive$disease, neighbors_two_param,
                                              list(eta_1 = 0, eta_2 = 0, beta_0 = 0, beta_1 = 0), n)

y_star_two_param_reg <- run_conclique_gibbs(concliques, neighbors_two_param, inits, "sampler_two_param_reg", params_est_two_param_reg, iter)
y_star_two_param_reg <- y_star_two_param_reg[(burnin:iter)[burnin:iter %% thin == 1], ]
gof_stat_star_two_param_reg <- rep(NA, B)
params_star_two_param_reg <- data.frame(eta_1 = rep(NA, B), eta_2 = rep(NA, B), beta_0 = rep(NA, B), beta_1 = rep(NA, B))

for(i in 1:B) {
  dat_two_param_reg <- y_star_two_param_reg[i,]
  params_star_two_reg <- fit_two_param_reg(dat_two_param_reg, neighbors_two_param,
                                           list(eta_1 = 0, eta_2 = 0, beta_0 = 0, beta_1 = 0), n)
  params_star_two_param_reg[i, ] <- c(params_star_two_reg$eta_1, params_star_two_reg$eta_2, params_star_two_reg$beta_0, params_star_two_reg$beta_1)
  resids_star_two_param_reg <- spatial_residuals(dat_two_param_reg, neighbors_two_param, "cdf_two_param_reg", params_star_two_reg, discrete = "pmf_two_param_reg", n)
  gof_stat_star_two_param_reg[i] <- gof_statistics(resids_star_two_param_reg, concliques, "ks", "max")
}

resids_two_param_reg <- spatial_residuals(endive$disease, neighbors_two_param, "cdf_two_param_reg", params_est_two_param_reg, discrete = "pmf_two_param_reg", n)
gof_stat_two_param_reg <- gof_statistics(resids_two_param_reg, concliques, "ks", "max")
p_value_two_param_reg <- (sum(gof_stat_star_two_param_reg >= gof_stat_two_param_reg) + 1)/(B + 1)
```
```{r endive-table, results='asis'}
endive_table0 <- data.frame(apply(params_star_one_param, 2, quantile, probs = c(.025, .5, .975)))
colnames(endive_table0) <- c("$\\eta$", "$\\kappa$")

endive_table1 <- data.frame(apply(params_star_two_param, 2, quantile, probs = c(.025, .5, .975)))
colnames(endive_table1) <- c("$\\eta_u$", "$\\eta_v$", "$\\kappa$")

endive_table2 <- data.frame(apply(params_star_two_param_reg, 2, quantile, probs = c(.025, .5, .975)))
colnames(endive_table2) <- c("$\\eta_u$", "$\\eta_v$", "$\\beta_0$", "$\\beta_1$")
rownames(endive_table2) <- paste0(unlist(strsplit(rownames(endive_table2), "%")), "\\%")

endive_table0 %>%
  bind_cols(endive_table1) %>%
  bind_cols(endive_table2) -> endive_table_all
rownames(endive_table_all) <- rownames(endive_table2)

endive_table_all %>%
  xtable(align = c("|", "l", "|", "r", "r", "|", "r", "r", "r", "|", "r", "r", "r", "r", "|"),
         digits = 3) -> endive_table


addtorow <- list()
addtorow$pos <- list(-1)
addtorow$command <- "\\hline &\\multicolumn{2}{|c|}{Model (1)} & \\multicolumn{3}{|c|}{Model (2)} & \\multicolumn{4}{|c|}{Model (3)} \\\\\n"
  
print(endive_table, add.to.row = addtorow, sanitize.text.function = function(x){x}, comment = FALSE, size="footnotesize")

```

Bootstrap percentile confidence intervals in all three autologistic models

# Sampling distributions of dependence parameters

```{r endive-param-plot, out.height='2.75in'}
# one param
ggplot() +
  geom_density(aes(params_star_one_param$eta), fill = "grey20", alpha = .5) +
  geom_vline(aes(xintercept = params_est_one_param$eta), colour = "red") +
  geom_text(aes(x = params_est_one_param$eta + diff(range(params_star_one_param$eta))/100,
                y = max(density(params_star_one_param$eta)$y)*.75,
                label = paste0("hat(eta) == " , round(params_est_one_param$eta, 4))),
            family = "serif", hjust = 0, parse = TRUE, size=6) +
  xlab("") +
  ylab("(1)") -> p1

# two param
t(unlist(params_est_two_param)) %>%
  data.frame() %>%
  select(starts_with("eta")) %>%
  rename(`eta[u]` = eta_1, `eta[v]` = eta_2) %>%
  gather(param, value) -> ests_two_param
  
params_star_two_param %>%
  select(starts_with("eta")) %>%
  rename(`eta[u]` = eta_1, `eta[v]` = eta_2) %>%
  gather(param, value) %>%
  ggplot() +
  geom_density(aes(value), fill = "grey20", alpha = .5) +
  geom_vline(aes(xintercept = value), colour = "red", data = ests_two_param) +
  geom_text(aes(x = value + diff(range(params_star_two_param$eta_1))/100,
                y = max(density(params_star_two_param$eta_1)$y)*.75,
                label = paste0("hat(eta) == " , round(value, 4))),
            family = "serif", hjust = 0, parse = TRUE, data = ests_two_param, size=6) +
  facet_wrap(~param, labeller = label_parsed) +
  xlab("") +
  ylab("(2)") -> p2

# two param reg
t(unlist(params_est_two_param_reg)) %>%
  data.frame() %>%
  select(starts_with("eta")) %>%
  rename(`eta[u]` = eta_1, `eta[v]` = eta_2) %>%
  gather(param, value) -> ests_two_param_reg
  
params_star_two_param_reg %>%
  select(starts_with("eta")) %>%
  rename(`eta[u]` = eta_1, `eta[v]` = eta_2) %>%
  gather(param, value) %>%
  ggplot() +
  geom_density(aes(value), fill = "grey20", alpha = .5) +
  geom_vline(aes(xintercept = value), colour = "red", data = ests_two_param_reg) +
  geom_text(aes(x = value + diff(range(params_star_two_param_reg$eta_1))/100,
                y = max(density(params_star_two_param_reg$eta_1)$y)*.75,
                label = paste0("hat(eta) == " , round(value, 4))),
            family = "serif", hjust = 0, parse = TRUE, data = ests_two_param_reg, size=6) +
  facet_wrap(~param, labeller = label_parsed) +
  xlab("") +
  ylab("(3)") -> p3

library(gridExtra)
grid.arrange(p1, p2, p3, ncol = 1)
```

# Common Spatial Simulation Approach

With common conditionally specified models for spatial lattice, standard MCMC simulation approach via Gibbs sampling is:

Starting from some initial $\boldsymbol Y_*^{(j)}\equiv\{Y_*^{(j)}(\boldsymbol s_1),\ldots,Y_*^{(j)}(\boldsymbol s_n)\}$,


1. Moving row-wise, for $i=1,\ldots,n$, individually simulate/update \red{$Y_*^{(j+1)}(\mbs_i)$} for each location \red{$\mbs_i$} from conditional cdf $F$ given
 \[\blue{Y_*^{(j+1)}(\mbs_1),\ldots,   Y_{*}^{(j+1)}(\mbs_{i-1})},  \quad Y_*^{(j)}(\mbs_{i+1}),\ldots,Y_*^{(j)}(\mbs_n)\]
    \setlength{\unitlength}{.1in}
    \hspace*{2cm}\begin{picture}(1,5)
    \multiput(1,1)(2,0){11}{\circle*{.8}}
    \multiput(1,3)(2,0){6}{\circle*{.8}}
    \multiput(13,3)(2,0){1}{\red{\circle*{.8}}}
    \multiput(15,3)(2,0){4}{\blue{\circle*{.8}}}
    \multiput(1,5)(2,0){11}{\blue{\circle*{.8}}}
    \thicklines
    \put(1,5){\blue{\line(1,0){20}}}
    \put(21,5){\blue{\line(0,-1){2}}}
    \put(21,3){\blue{\vector(-1,0){7.6}}}
    \end{picture}
2.  $n$ individual updates provide 1 full Gibbs iteration.
3. Repeat 1-2 to obtain  $M$ resampled spatial data sets $\mbY_*^{(j)}$, $j=1,\ldots,M$ (e.g., can burn-in, thin, etc.)

# Endive data timing

```{r endive-timing, cache=TRUE}
time <- Sys.time()
tmp <- run_conclique_gibbs(concliques, neighbors_one_param, inits, "binary_single_param", params_est_one_param, 100)
conc_time_m0 <- difftime(Sys.time(), time, units = "secs")

time <- Sys.time()
tmp <- run_conclique_gibbs(concliques, neighbors_two_param, inits, "binary_two_param", params_est_two_param, 100)
conc_time_m1 <- difftime(Sys.time(), time, units = "secs")

time <- Sys.time()
tmp <- run_conclique_gibbs(concliques, neighbors_two_param, inits, "sampler_two_param_reg", params_est_two_param_reg, 100)
conc_time_m2 <- difftime(Sys.time(), time, units = "secs")

time <- Sys.time()
tmp <- run_sequential_gibbs(neighbors_one_param, inits, "binary_single_param", params_est_one_param, 100)
seq_time_m0 <- difftime(Sys.time(), time, units = "secs")

time <- Sys.time()
tmp <- run_sequential_gibbs(neighbors_two_param, inits, "binary_two_param", params_est_two_param, 100)
seq_time_m1 <- difftime(Sys.time(), time, units = "secs")

time <- Sys.time()
tmp <- run_sequential_gibbs(neighbors_two_param, inits, "sampler_two_param_reg", params_est_two_param_reg, 100)
seq_time_m2 <- difftime(Sys.time(), time, units = "secs")
```

- Endive example dataset simulations performed with the proposed (conclique-based) Gibbs sampler (to follow)
- Reported results would have been virtually identical with the same number of iterations to the standard sequential Gibbs sampler
- Generation of the reference distribution using the standard sampler would have taken approximately 
    1. $`r round((seq_time_m0 - conc_time_m0)/100*B/60, 2)`$ minutes longer  
    2. $`r round((seq_time_m1 - conc_time_m1)/100*B/60, 2)`$ minutes longer
    3. $`r round((seq_time_m2 - conc_time_m2)/100*B/60, 2)`$ minutes longer 
- Conclique MRF sampler had running times 
    1. $`r round((conc_time_m0)/100*B, 2)`$ seconds
    2. $`r round((conc_time_m1)/100*B, 2)`$ seconds
    3. $`r round((conc_time_m2)/100*B, 2)`$ seconds


# Concliques

\begin{block}{Cliques -- Hammersley and Clifford (1971)}
Singletons and sets of locations such that each location in the set is a neighbor of all other locations in the set \\
Example: Four nearest neighbors gives cliques of sizes $1$ and $2$
\end{block}

\begin{block}{The Converse of Cliques -- Concliques (Kaiser, Lahiri, and Nordman 2012)}
Sets of locations such that no location in the set is a neighbor of any other location in the set

\vspace{-.5cm}

\footnotesize
\begin{columns}
\hspace*{-.8cm}
\begin{column}{0.1\textwidth}
\begin{center}
\vspace*{-.9cm}
\underline{4 Nearest}\\ \underline{Neighbors}
$$
\begin{array}{ccc}
\cdot&*&\cdot \\
*&\mbs&* \\
\cdot&*&\cdot \\
\end{array}
$$
\end{center}
\end{column}
\begin{column}{0.1\textwidth}
\begin{center}
 \underline{Concliques}\\ \underline{4 Nearest}\\ \underline{Neighbors}
$$
\begin{array}{cccc}
1&2&1&2 \\
2&1&2&1 \\
1&2&1&2 \\
2&1&2&1 \\
\end{array}
$$
\end{center}
\end{column}
\begin{column}{0.1\textwidth}
\begin{center}
\vspace*{-.9cm}
\underline{8 Nearest}\\ \underline{Neighbors}
$$
\begin{array}{ccc}
*&*&* \\
*&\mbs&* \\
*&*&* \\
\end{array}
$$
\end{center}
\end{column}
\begin{column}{0.1\textwidth}
\begin{center}

\vspace*{-.2cm}
 \underline{Concliques}\\ \underline{8 Nearest}\\ \underline{Neighbors}
$$
\begin{array}{cccc}
1&2&1&2 \\
3&4&3&4 \\
1&2&1&2 \\
3&4&3&4 \\
\end{array}
$$
\end{center}
\end{column}
\end{columns}
\end{block}

# Generalized spatial residuals @kaiser2012goodness

\begin{block}{Definition}
\begin{itemize}
%\item Let ${\cal R}$ be a spatial lattice on which variables are defined (may be irregular in shape)
\item   $F(y|\mby(\mathcal{N}_i), \mbtheta)$ is the conditional cdf of $Y(\mbs_i)$ under the model
\item Substitute random variables, $Y(\mbs_i)$ and neighbors $\{Y(\mbs_j): \mbs_j \in \mathcal{N}_i \}$, into  (continuous) conditional cdf to define residuals:
\begin{displaymath}
R(\mbs_i) = F(Y(\mbs_i)|\{Y(\mbs_j): \mbs_j \in \mathcal{N}_i \}, \mbtheta).
\end{displaymath}
\end{itemize}
\end{block}

\begin{block}{Key Property}
Let $\{ {\cal C}_j: \, j=1, \ldots, q \}$ be a collection of concliques that partition the integer grid. Under the conditional model, \textbf{spatial residuals {\it within} a conclique are iid  Uniform$(0, 1)$-distributed}: \\
\begin{displaymath}
\{ R(\mbs_i): \, \mbs_i \in {\cal C}_j \} \stackrel{iid}{\sim} \mbox{ Uniform}(0, 1) \qquad \text{ for } j=1, \dots, q
\end{displaymath}
\end{block}

<!-- @kaiser2012goodness considered this residual result for model assessment (GOF), but the same result suggests a new simulation device from concliques  -->

# Conclique-based Gibbs sampler

Using the conditional independence of random variables at locations within a conclique we propose a conclique-based Gibbs sampling algorithm for sampling from a MRF.

1. Split locations into $Q$ disjoint concliques, $\mathcal{D} = \cup_{i = 1}^Q\mathcal{C}_i$.
1. Initialize the values of $\{Y^{(0)}(\boldsymbol s): \boldsymbol s \in \{\mathcal{C}_2, \dots, \mathcal{C}_Q\}\}$.
1. Starting from $\mathcal{C}_1$ for the $i^{th}$ iteration, draw $\{ Y^{(i)}(\boldsymbol s) : \boldsymbol s\in \mathcal{C}_1$\}  as random sample where $Y^{(i)}(\boldsymbol s) \stackrel{iid}{\sim} F(y(\boldsymbol s)|Y^{(i-1)}(\boldsymbol t), \boldsymbol t \in \mathcal{N}(\boldsymbol s))$
1. Update observations conclique-wise (using previous conclique updates). 
    - For $j=2,\ldots,Q$, draw $\{ Y^{(i)}(\boldsymbol s) : \boldsymbol s\in \mathcal{C}_j$\}  as random sample where $Y^{(i)}(\boldsymbol s) \stackrel{iid}{\sim} F(y(\boldsymbol s)|\{Y^{(i)}(\boldsymbol t), \boldsymbol t \in \mathcal{N}(\boldsymbol s) \cap \mathcal{C}_k \text{ where } k < j\}, \{Y^{(i-1)}(\boldsymbol t), \boldsymbol t \in \mathcal{N}(\boldsymbol s) \cap \mathcal{C}_k \text{ where } k > j\})$

This works by conditional independence & because neighbors for updating one conclique always belong to other concliques.   

# It's (computationally) fast!

- Because we are using batch updating vs. standard (i.e., single-location-wise) updating in a Gibbs sampler, this approach is **computationally fast**
\vfill
- A flexible `R` package using `Rcpp` (called `conclique`, to appear on CRAN) that implements a conclique-based Gibbs sampler while allowing the user to specify an arbitrary model.
\vfill
- More numerical comparisons to the standard Gibbs to follow 

# It's (provably) fast!

- While computationally fast, the MCMC sampler is also provably geometrically ergodic (i.e., the MCMC mixes at a fast rate) in a general sense, which is unusual for spatial data.
\vspace{.2in}
- State-of-the-art general theory for proving geometric ergodicity of Gibbs samplers exists only for two-state samplers (i.e., drift & minorization conditions) [@johnson2015geometric].
    \vspace{.1in}
    - For common 4-nearest neighbor spatial models, there are exactly 2 concliques (two stages in the conclique-based Gibbs sampler).
    \vspace{.1in}
    - One can formally prove that the spatial sampler proposed is geometrically ergodic for many conditional spatial models (Gaussian, Gamma, Inverse-gamma, Beta, Binomial, etc.)      

# The conclique-based Gibbs sampler works

\begin{block}{Conclique positivity condition}
\label{condition:conc-pos}
The full conditionals for the MRF model specify a valid joint distribution $\Pi(\cdot)$ for $(Y(\boldsymbol s_1), \dots, Y(\boldsymbol s_n))$ with density/mass function $\pi(\cdot)$ having support $\mathcal{X} \subset \mathbb{R}^n$. For the collected $\mathcal{C}_1, \dots, \mathcal{C}_Q$ of concliques under the MRF model, it holds that
$\mathcal{X} = \mathcal{X}_1 \times \cdots \times \mathcal{X}_Q$ where $\mathcal{X}_i$ denotes the support of the marginal density of observations $\{Y(s_j): s_j \in \mathcal{C}_i\}$ with locations in conclique $\mathcal{C}_i, i=1, \dots, Q$.
\end{block}

\begin{theorem}
\label{thm:gibbs_harris}
Suppose the conclique positivity condition holds. Then the conclique-based Gibbs sampler is Harris ergodic (i.e., $phi$-irreducible, aperiodic and Harris recurrent) with stationary distribution given by the joint data distribution, $\Pi(\cdot)$ and, for any initialization $x\in\mathcal{X}$, the sampler will monotonically converge to $\Pi(\cdot)$ in total variation distance as the number of iterations $m\to \infty$, i.e.,
$
\sup_{A \in \mathcal{F}}| P^{(m)}(x,A) -\Pi(A)| \downarrow 0 \text{ as } m \to \infty.
$
\end{theorem}

# Geometric ergodicity

- $\Pi(\cdot)$ the joint distribution of observations $\{Y(\boldsymbol s_1),\ldots,Y(\boldsymbol s_n)\}$ induced by a MRF specification 
- $P^{(m)}(x, \cdot)$ the transition distribution at the $m$th iteration of the sampler with initialization $x\in\mathcal{X}$ 

\begin{block}{Geometric ergodicity}
The sampler is \emph{geometrically ergodic} if there exists some real-valued function $G: \mathcal{X} \rightarrow \mathbb{R}$ and some constant $t \in (0,1)$ which satisfy
$$
\sup_{A \in \mathcal{F}}\vert P^{(m)}(x, A) - \Pi(A) \vert \le G(x)t^m \text{  for any } x \in \mathcal{X},
$$
where $\mathcal{F}$ denotes the $\sigma$-algebra associated with the joint support $\mathcal{X}\subset \mathbb{R}^n$.
\end{block}

# Theoretical results

\begin{theorem}
\label{thm:2}
Suppose a MRF model for $\{Y(\boldsymbol s_i): i = 1, \dots, n\}$ admits two concliques and assume the conclique positivity condition holds with $\mathcal{X} = \mathcal{X}_1\times\mathcal{X}_2 \subset \mathbb{R}^n$ (where $\mathcal{X}_i$ denotes the support of observations associated wit conclique $i = 1,2$). Additionally, suppose that either $\mathcal{X}_1$ or $\mathcal{X}_2$ is compact and that the full conditionals are continuous in conditioning variables $\boldsymbol y(\mathcal{N}_i)$, $i = 1, \dots, n$. Then, the conclique-based Gibbs sampler (CGS) is geometrically ergodic with stationary distribution given by the full joint, $\Pi(\cdot)$. 
\end{theorem}

Ensures geometric ergodicity of the conclique-based Gibbs sampler for several four-nearest neighbor MRF models including (1) the autologistic binary, (2) the conditional binomial, (3) the conditional Beta, and (4) the Multinomial distributions as well as (5) the windsorized Poisson model of @kaiser1997modeling.

# Theoretical results (cont'd)

Geometric ergodicity of the conclique-based Gibbs sampling algorithm can also be established for four-nearest neighborhood MRF models with unbounded support

\vfill

\begin{theorem}
\label{thm:cases}
Suppose $\{Y(\boldsymbol s_i): i = 1, \dots, n\}$ with positions on a regular lattice in $\mathbb{R}^2$ follow a MRF model with a exponential family form and a four-nearest neighborhood structure. Then, the conclique-based Gibbs sampler (CGS) is geometrically ergodic for the following cases.
\end{theorem}

# Theoretical results (cont'd)

\begin{block}{}
\begin{enumerate}[(a)]
\item The conditional Gaussian model having conditional variance $\tau^2$ and density 
$$
f_i(y(\boldsymbol s_i)|\boldsymbol y(\mathcal{N}_i)) = \frac{1}{\sqrt{2\pi}\tau}\exp\left\{-\frac{1}{2\tau^2}(y(\boldsymbol s_i) - \mu(\boldsymbol s_i))\right\}, \quad y(\boldsymbol s_i) \in \mathbb{R},
$$
and conditional mean
$$
\mu(\boldsymbol s_i) = \alpha + \eta\sum\limits_{s_j \in \mathcal{N}_i}\{y(s_j) - \alpha\}
$$
where $|\eta| < 0.25$ and $\alpha \in \mathbb{R}$.
\end{enumerate}
\end{block}

# Theoretical results (cont'd)

\begin{block}{}
\begin{enumerate}[(a)]
\setcounter{enumi}{1}
\item The conditional (centered) Inverse Gaussian model with conditional density form
\scriptsize
$$
f_i(y_i | \boldsymbol \theta) = \exp \left\{\frac{A_{1i}(\boldsymbol y(\mathcal{N}_i))}{2} y(\boldsymbol s_i) - \frac{A_{2i}(\boldsymbol y(\mathcal{N}_i))}{2} \frac{1}{y(\boldsymbol s_i)} -B_i(\boldsymbol y(\mathcal{N}_i)) + C(y(\boldsymbol s_i))\right\}, \text{ } y(\boldsymbol s_i) \ge 1
$$
\normalsize
where
\begin{align*}
A_{1i}(\boldsymbol y(\mathcal{N}_i)) &= \frac{\lambda}{\mu^2} + \eta_1 \sum\limits_{\boldsymbol s_j \in \mathcal{N}_i}\left(\frac{1}{y(\boldsymbol s_j)} - \frac{1}{\mu} - \frac{1}{\lambda}\right) \\
A_{2i}(\boldsymbol y(\mathcal{N}_i)) &= \lambda + \eta_2 \sum\limits_{\boldsymbol s_j \in \mathcal{N}_i}\left(y(\boldsymbol s_j) - \mu \right) \\
\end{align*}
and $\mu, \lambda > 0$, $0 \le \eta_1 \le \lambda^2/4\mu(\lambda + \mu), 0 \le \eta_2 \le \lambda^2/4\mu$.
\end{enumerate}
\end{block}

# Theoretical results (cont'd)

\begin{block}{}
\begin{enumerate}[(a)]
\setcounter{enumi}{2}
\item The conditional (centered) Truncated Gamma model with conditional density
\footnotesize
$$
f_i(y(\boldsymbol s_i) | \boldsymbol \theta) = \exp \left\{A_{1i}(\boldsymbol y(\mathcal{N}_i)) \log(y_i) - A_{2i}(\boldsymbol y(\mathcal{N}_i)) y_i -B_i(\boldsymbol y(\mathcal{N}_i))) \right\}, \text{ } y(\boldsymbol s_i) \ge 1
$$
\normalsize
where 
$$
A_{1i}(\boldsymbol y(\mathcal{N}_i)) = \alpha_1 + \eta \sum\limits_{\boldsymbol s_j \in \mathcal{N}_i}\log(y(\boldsymbol s_j)) \quad \text{and} \quad A_{2i}(\boldsymbol y(\mathcal{N}_i)) = \alpha_2 
$$
for $\eta > 0, \alpha_1 >-1,  \alpha_2 > 0$.
\end{enumerate}
\end{block}


# Simulation comparisons

Quantitative framework from @turek2017automated to compare conclique-based and sequential Gibbs sampler efficiency

1. Mixing effectiveness (algorithmic efficiency)
2. Computational demands of the algorithm (computational efficiency)

**Algorithmic efficiency criterion:**
$$
A = \min\limits_{1 \le i \le n}\left\{\left( 1 + 2\sum\limits_{j = 1}^\infty \rho_i(j)\right)^{-1}\right\}
$$

**Computational efficiency criterion:**
\vspace{-.2in}
\begin{align*}
C &= \begin{cases}
\sum\limits_{k = 1}^Q \text{samp}(\{Y(\boldsymbol s_i) : \boldsymbol s_i \in \mathcal{C}_k\}| \mathcal{C}_{j}, j \not= k) & \text{Conclique-based}\\
\sum\limits_{k = 1}^n \text{samp}(Y(\boldsymbol s_k) | Y(\boldsymbol s_j), j \not= k) & \text{Sequential}
\end{cases}
\end{align*}

# Simulation comparisons (Cont'd)

Going back to simulation from 3 binary models for spatial endive data 

```{r algo-eff, cache=TRUE}
N <- 40
params0 <- list(eta = .5, kappa = .5)
params1 <- list(eta_1 = .2, eta_2 = .7, kappa = .5)
params2 <- list(eta_1 = .2, eta_2 = .7, beta_0 = -10, beta_1 = .5)
num_sample <- 10000

#create grid, lattices, neighbors, concliques for MCMC ------------------
grid <- matrix(1:(N*N), nrow = N)
conclique <- concliques_4nn(grid)
lattice <- lattice_4nn_torus(dimvec = c(N, N))
neighbors <- get_neighbors(lattice, TRUE, grid)

## custom function -------
Rcpp::sourceCpp("../code/binary_two_param_reg_sampler.cpp")

## repeat ----------
times <- 10
algo_eff <- list(conc = list(m0 = rep(NA, times), m1 = rep(NA, times), m2 = rep(NA, times)),
                 seq = list(m0 = rep(NA, times), m1 = rep(NA, times), m2 = rep(NA, times)))

## do one for later -----
inits <- matrix(rbinom(n = N*N, size = 1, prob = .5), nrow = N)
samp_conc_m0 <- run_conclique_gibbs(conclique, neighbors, inits, "binary_single_param", params0, num_sample)
samp_seq_m0 <- run_sequential_gibbs(neighbors, inits, "binary_single_param", params0, num_sample)
samp_conc_m1 <- run_conclique_gibbs(conclique, neighbors, inits, "binary_two_param", params1, num_sample)
samp_seq_m1 <- run_sequential_gibbs(neighbors, inits, "binary_two_param", params1, num_sample)
samp_conc_m2 <- run_conclique_gibbs(conclique, neighbors, inits, "binary_two_param_reg_sampler", params2, num_sample)
samp_seq_m2 <- run_sequential_gibbs(neighbors, inits, "binary_two_param_reg_sampler", params2, num_sample)

## get A -------
algo_eff$conc$m0[1] <- min(apply(samp_conc_m0, 2, LaplacesDemon::IAT)^(-1))
algo_eff$seq$m0[1] <- min(apply(samp_seq_m0, 2, LaplacesDemon::IAT)^(-1))
algo_eff$conc$m1[1] <- min(apply(samp_conc_m1, 2, LaplacesDemon::IAT)^(-1))
algo_eff$seq$m1[1] <- min(apply(samp_seq_m1, 2, LaplacesDemon::IAT)^(-1))
algo_eff$conc$m2[1] <- min(apply(samp_conc_m2, 2, LaplacesDemon::IAT)^(-1), na.rm = TRUE)
algo_eff$seq$m2[1] <- min(apply(samp_seq_m2, 2, LaplacesDemon::IAT)^(-1), na.rm = TRUE)

## do the rest -----
for(i in 2:times) {
  ## get samples ----------
  inits <- matrix(rbinom(n = N*N, size = 1, prob = .5), nrow = N)
  samp_conc_m0 <- run_conclique_gibbs(conclique, neighbors, inits, "binary_single_param", params0, num_sample)
  samp_seq_m0 <- run_sequential_gibbs(neighbors, inits, "binary_single_param", params0, num_sample)
  samp_conc_m1 <- run_conclique_gibbs(conclique, neighbors, inits, "binary_two_param", params1, num_sample)
  samp_seq_m1 <- run_sequential_gibbs(neighbors, inits, "binary_two_param", params1, num_sample)
  samp_conc_m2 <- run_conclique_gibbs(conclique, neighbors, inits, "binary_two_param_reg_sampler", params2, num_sample)
  samp_seq_m2 <- run_sequential_gibbs(neighbors, inits, "binary_two_param_reg_sampler", params2, num_sample)
  
  ## get A -------
  algo_eff$conc$m0[i] <- min(apply(samp_conc_m0, 2, LaplacesDemon::IAT)^(-1))
  algo_eff$seq$m0[i] <- min(apply(samp_seq_m0, 2, LaplacesDemon::IAT)^(-1))
  algo_eff$conc$m1[i] <- min(apply(samp_conc_m1, 2, LaplacesDemon::IAT)^(-1))
  algo_eff$seq$m1[i] <- min(apply(samp_seq_m1, 2, LaplacesDemon::IAT)^(-1))
  algo_eff$conc$m2[i] <- min(apply(samp_conc_m2, 2, LaplacesDemon::IAT)^(-1), na.rm = TRUE)
  algo_eff$seq$m2[i] <- min(apply(samp_seq_m2, 2, LaplacesDemon::IAT)^(-1), na.rm = TRUE)
}
```

```{r compu-eff-format, cache=TRUE}
get_data <- function(data, neighbors, gibbs = c("conclique", "sequential"), concliques = NULL, N = NULL) {
  stopifnot((gibbs == "conclique" & !is.null(concliques)) | gibbs == "sequential")
  
  nums <- lapply(neighbors, function(neigh) {
    rowSums(!is.na(neigh[, -1]))
  }) 
  sums <- lapply(neighbors, function(neigh) {
    rowSums(matrix(data[neigh[, -1]], ncol = ncol(neigh) - 1, byrow = TRUE))
  }) 
  u <- (0:(length(data) - 1)) %% N + 1
  
  
  if(gibbs == "sequential") {
    res <- lapply(seq_along(data), function(i) list(data = data[i], nums = lapply(nums, function(num) num[i]),
                                                   sums = lapply(sums, function(sum) sum[i]), u = u[i]))
  } else {
    res <- lapply(concliques, function(conc) {
      list(data = data[conc], 
           nums = lapply(nums, function(num) num[conc]),
           sums = lapply(sums, function(sum) sum[conc]), 
           u = u[conc])
    })
  }
  return(res)
}

format_conc_m0 <- lapply(seq_len(nrow(samp_conc_m0)), function(row) {
  get_data(samp_conc_m0[row, ], neighbors, "conclique", conclique, N)
  }) %>%
  unlist(recursive = FALSE)
format_seq_m0 <- lapply(1:10, function(row) {
  get_data(samp_seq_m0[row, ], neighbors, "sequential", conclique, N)
  }) %>%
  unlist(recursive = FALSE)
format_conc_m1 <- lapply(seq_len(nrow(samp_conc_m1)), function(row) {
  get_data(samp_conc_m1[row, ], neighbors, "conclique", conclique, N)
  }) %>%
  unlist(recursive = FALSE)
format_seq_m1 <- lapply(1:10, function(row) {
  get_data(samp_seq_m1[row, ], neighbors, "sequential", conclique, N)
  }) %>%
  unlist(recursive = FALSE)
format_conc_m2 <- lapply(seq_len(nrow(samp_conc_m2)), function(row) {
  get_data(samp_conc_m2[row, ], neighbors, "conclique", conclique, N)
  }) %>%
  unlist(recursive = FALSE)
format_seq_m2 <- lapply(1:10, function(row) {
  get_data(samp_seq_m2[row, ], neighbors, "sequential", conclique, N)
  }) %>%
  unlist(recursive = FALSE)
```

```{r compu-eff-times, cache=TRUE}
times_conc_m0 <- lapply(format_conc_m0, function(x) {
  time <- Sys.time()
  tmp <- binary_single_param_sampler(x, params0)
  time2 <- Sys.time()
  as.numeric(difftime(time2, time, units = "secs"))
}) %>% unlist()

times_seq_m0 <- lapply(format_seq_m0, function(x) {
  time <- Sys.time()
  tmp <- binary_single_param_sampler(x, params0)
  time2 <- Sys.time()
  as.numeric(difftime(time2, time, units = "secs"))
}) %>% unlist()

times_conc_m1 <- lapply(format_conc_m1, function(x) {
  time <- Sys.time()
  tmp <- binary_two_param_sampler(x, params1)
  time2 <- Sys.time()
  as.numeric(difftime(time2, time, units = "secs"))
}) %>% unlist()

times_seq_m1 <- lapply(format_seq_m1, function(x) {
  time <- Sys.time()
  tmp <- binary_two_param_sampler(x, params1)
  time2 <- Sys.time()
  as.numeric(difftime(time2, time, units = "secs"))
}) %>% unlist()

times_conc_m2 <- lapply(format_conc_m2, function(x) {
  time <- Sys.time()
  tmp <- binary_two_param_reg_sampler(x, params2)
  time2 <- Sys.time()
  as.numeric(difftime(time2, time, units = "secs"))
}) %>% unlist()

times_seq_m2 <- lapply(format_seq_m2, function(x) {
  time <- Sys.time()
  tmp <- binary_two_param_reg_sampler(x, params2)
  time2 <- Sys.time()
  as.numeric(difftime(time2, time, units = "secs"))
}) %>% unlist()

```

\begin{table}[ht]
\centering
\begin{tabular}{|l|rr|rr|rr|}
\hline
Gibbs & \multicolumn{2}{|c|}{Model (1)} & \multicolumn{2}{|c|}{Model (2)} & \multicolumn{2}{|c|}{Model (3)} \\
\cline{2-7}
 & A & C & A & C & A & C \\
\hline
Conclique & $`r round(mean(algo_eff$conc$m0), 3)`$ & $`r round(mean(times_conc_m0)*length(conclique), 5)`$ & $`r round(mean(algo_eff$conc$m1), 3)`$ & $`r round(mean(times_conc_m1)*length(conclique), 5)`$ & $`r round(mean(algo_eff$conc$m2), 3)`$ & $`r round(mean(times_conc_m2)*length(conclique), 5)`$ \\
Standard & $`r round(mean(algo_eff$seq$m0), 3)`$ & $`r round(mean(times_seq_m0)*ncol(samp_conc_m0), 3)`$ & $`r round(mean(algo_eff$seq$m1), 3)`$ & $`r round(mean(times_seq_m1)*ncol(samp_conc_m0), 3)`$ & $`r round(mean(algo_eff$seq$m2), 3)`$ & $`r round(mean(times_seq_m2)*ncol(samp_conc_m0), 3)`$ \\
\hline
\end{tabular}
\label{tab:eff-results}
\end{table}

- Measures of algorithmic and computational efficiency, $A$ and $C$, for three autologistic models on a $`r N` \times `r N`$ grid
- Estimates of $A$ determined by average from $10$ chains ($10,000$ iter)
- Estimates of $C$ determined by average running times of 
    - $20,000$ conclique-based $\text{samp}(\{ Y(\boldsymbol s) : \boldsymbol s \in \mathcal{C}_k\})$ 
    - $16,000$ sequential $\text{samp}(Y(\boldsymbol s_k) | Y(\boldsymbol s_j), j \not= k)$ 

<!-- Here as in other numerical studies, there's no strong evidence that, for a fixed number of iterations, Conclique/standard mix any differently. -->

<!-- Though conclique is faster at producing many iterations &, as the model gets more complex, need more iterations to improve effect sample size  -->

# Timing simulations

Comparisons of log time for simulation of $M=100, 1000, 5000, 10000$ four-nearest neighbor Gaussian MRF datasets on a lattice of size $m \times m$ for various size grids, $m = 5, 10, 20, 30, 50, 75$, using sequential and conclique-based Gibbs samplers

```{r timings, fig.height=4, fig.height=3}
load("../data/6_timings.RData")

summary_times <- timings %>% 
  filter(N == 75) %>% 
  gather(gibbs, time, conclique, sequential) %>% 
  group_by(gibbs, n.iter) %>% 
  summarise(mean_time = mean(time)) %>%
  spread(gibbs, mean_time)

timings %>%
  gather(gibbs, time, conclique, sequential) %>% ungroup() %>%
  mutate(n.iter_f = factor(paste("M =", n.iter), levels=unique(paste("M =", timings$n.iter)))) %>%
  group_by(N, n.iter, gibbs) %>%
  mutate(mean_time = mean(time)) %>%
  ggplot() +
  geom_jitter(aes(N, log(time), colour = gibbs), alpha = .1, size = 2) +
  geom_line(aes(N, log(mean_time), colour = gibbs), size = 1) +
  facet_wrap(~n.iter_f, nrow = 1) +
  xlab("m") +
  ylab("Log Time (seconds)") +
  scale_colour_discrete("Gibbs sampler", labels=c("Conclique", "Sequential"))
```

For $10,000$ iterations/samples on $75 \times 75$ grid, conclique-based took $`r round(summary_times$conclique[4], 2)`$ seconds and sequential took $`r round(summary_times$sequential[4], 2)`$ seconds $\approx `r round(summary_times$sequential[4]/(60*60), 2)`$ hours.

# Another application example (Goodness of Fit)

An important question for Markov random field models with spatial data is 

> How to assess/diagnose fit? 

\begin{block}{Composite Hypothesis}
\vspace{-.2in}
\begin{align*}
H_0(C):& \text{ The conditional distributions of } \{Y(\boldsymbol s_i): i=1, \ldots, n\} \nonumber \\
  & \text{ are } F(y(\boldsymbol s_i)|\boldsymbol y(\mathcal{N}_i), \boldsymbol \theta)
\end{align*}
where $\boldsymbol \theta \in \Theta$ is some \emph{unknown} parameter value
\end{block}

- @kaiser2012goodness provide a methodology for performing GOF tests using concliques
- Conclique-based Gibbs sampling allows for fast approximation of the reference distribution for the GOF test statistics in this methodology

# Simple example

\begin{block}{Gaussian Conditional Model - $20 \times 20$ Lattice, 4-nearest Neighbors}
Let $Y(\boldsymbol s_i) | \boldsymbol y(\mathcal{N}_i) \sim N(\mu(\boldsymbol s_i), \tau^2)$, where $ \mu(\boldsymbol s_i) = \alpha + \eta\sum\limits_{\boldsymbol s_j \in \mathcal{N}_i}(y(\boldsymbol s_j) - \alpha)$.

Truth: $\alpha = 10, \tau^2 = 2, \eta = 0.24.$
\end{block}

```{r}
N <- 20
params <- list(rho = sqrt(2), kappa = 10, eta = .24)


#create concliques list -------------------------------
concliques_4nn <- function(grid) {
  concliques <- list()
  concliques[[1]] <- grid[(row(grid) %% 2 == 1 & col(grid) %% 2 == 1) | (row(grid) %% 2 == 0 & col(grid) %% 2 == 0)]
  concliques[[2]] <- grid[(row(grid) %% 2 == 0 & col(grid) %% 2 == 1) | (row(grid) %% 2 == 1 & col(grid) %% 2 == 0)]
  class(concliques) <- "conclique_cover"
  return(concliques)
}

#create grid, lattices, neighbors, concliques for MCMC ------------------
grid <- matrix(1:(N*N), nrow = N)
concliques <- concliques_4nn(grid)
lattice <- lattice_4nn_torus(dimvec = c(N, N))
inits <- matrix(rbinom(n = N*N, size = 1, prob = .5), nrow = N)
neighbors <- get_neighbors(lattice)

#simulate from Normal directly-------------------
make_IminusC_inv <- function(eta, N) {
  C <- matrix(0, nrow = N^2, ncol = N^2)
  for(i in 1:N^2) {
    C[i, neighbors[[1]][i, -1]] <- eta
  }
  I_minus_C <- diag(nrow = N^2, ncol = N^2) - C
  inv <- try(solve(I_minus_C), silent = TRUE)
  return(inv)
}
IminusC_inv <- make_IminusC_inv(params$eta, N)
covar <- IminusC_inv %*% diag(params$rho^2, N^2)
mu <- rep(params$kappa, N^2)
conclique_dat <- rmvnorm(1, mu, covar)


##good model ------
conclique_resids <- spatial_residuals(conclique_dat, neighbors, "gaussian_single_param", params)
ecdf_vals <- lapply(concliques, function(conc) {fn <- ecdf(conclique_resids[conc]); fn(conclique_resids[conc])})
names(ecdf_vals) <- 1:length(concliques)
ecdf_vals <- do.call(cbind, ecdf_vals) %>% data.frame()

ecdf_vals %>%
  data.frame() %>%
  gather(conclique, ecdf) %>%
  separate(conclique, into = c("junk", "conclique"), 1) -> ecdf_vals

ecdf_vals[, "u"] <- do.call(c, lapply(concliques, function(conc) conclique_resids[conc]))

##bad model -----------
conclique_resids_err <- spatial_residuals(conclique_dat, neighbors, "gaussian_single_param", list(rho = sqrt(2), kappa = 10, eta = -0.1))

ecdf_vals_err <- lapply(concliques, function(conc) {fn <- ecdf(conclique_resids_err[conc]); fn(conclique_resids_err[conc])})
names(ecdf_vals_err) <- 1:length(concliques)
ecdf_vals_err <- do.call(cbind, ecdf_vals_err) %>% data.frame()

ecdf_vals_err %>%
  data.frame() %>%
  gather(conclique, ecdf) %>%
  separate(conclique, into = c("junk", "conclique"), 1) -> ecdf_vals_err

ecdf_vals_err[, "u"] <- do.call(c, lapply(concliques, function(conc) conclique_resids_err[conc]))
```

```{r conc-fig, fig.show='hold', fig.height=2}
ecdf_vals %>%
  ggplot() +
  geom_point(aes(u, ecdf, colour = conclique), size = 1) +
  geom_abline(aes(slope = 1, intercept = 0)) +
  theme(aspect.ratio = 1, legend.position="none") +
  ggtitle(expression(paste(eta, " = 0.24, (correct)"))) +
  ylim(c(0, 1)) + xlim(c(0, 1))


ecdf_vals_err %>%
  ggplot() +
  geom_point(aes(u, ecdf, colour = conclique), size = 1) +
  geom_abline(aes(slope = 1, intercept = 0)) +
  theme(aspect.ratio = 1) +
  ggtitle(expression(paste(eta, " = -0.10, (incorrect)"))) +
  ylim(c(0, 1)) + xlim(c(0, 1))
```

# From residuals to test statistics

\begin{block}{Residual Empirical Distribution}
Divide locations $\{\boldsymbol s_i\}_{i=1}^n$ into concliques:  $\mathcal{C}_{j}$, $j=1,\ldots,q$

For $j^{th}$ conclique, empirical cdf and and its difference to Uniform$(0,1)$ cdf

\begin{displaymath}
G_{jn}(u) = \frac{1}{|{\cal C}_{j}|} \sum_{\boldsymbol s_i \in {\cal C}_{j}} I [ R(\mbs_i) \leq u]
\end{displaymath}
\vspace{-.4cm}
\begin{displaymath}
W_{jn}(u) \equiv n^{1/2} \left[ G_{jn}(u) - u \right]; \, \, \, \, \, u \in [0,1]
\end{displaymath}
\end{block}

\begin{block}{Test Statistics}
\vspace{-.4in}
\begin{eqnarray}
T_{1n} & = & \max_{j=1, \ldots, q} \sup_{u \in [0,1]} | W_{jn}(u) | \nonumber \\
T_{2n} & = &  \frac{1}{q} \sum_{j=1}^{q} \left( \int_0^1 | W_{jn}(u) |^2 du \right)^{1/2}  \nonumber
\end{eqnarray}
\end{block}

Asymptotic behavior of test statistics $T_{kn}$ is non-trivial (resampling is helpful to approximate distributions)

# GOF methodology in practice

In application, a conditional distribution $F$ model is formulated/specified.

1. Fit model $\hat{\boldsymbol \theta}$ to original data $Y_1,\ldots,Y_n$
2. Compute generalized residuals and test statistics: $T_{kn}$
3. Simulate spatial data $Y_1^*,\ldots,Y_n^*$ from fitted cond. cdf: $F_{\hat{\boldsymbol \theta}}$
4. Fit model to simulated data:  $\hat{\boldsymbol \theta}^*$
5. Compute generalized residuals and test statistics: $T_{kn}^*$ from $Y_1^*,\ldots,Y_n^*$ and $F_{\hat{\boldsymbol \theta}^*}$
6. Do 3-5 many times
7. Result is reference distribution for test statistic  $T_{kn}$

In simulating/resampling step 3 for spatial data, can use \blue{conclique-based Gibbs sampler} due to the conditional specification $F$ for each location.


# Simulated example

```{r lognormal, results='hide'}
fit_gauss_4nn <- function(data, neighbors, params0) {
  # score functions
  score_fns <- function(params, data) {
    y <- data$y
    sums <- data$sums
    nums <- data$nums
    
    kappa <- params[1]
    eta <- params[2]
    
    mu <- kappa + eta*sums - nums*kappa*eta
    
    c(sum((y - mu)*(1 - nums*eta)), sum((y - mu)*(sums - nums*kappa)))
  }
  rho2_hat <- function(kappa_hat, eta_hat, data) {
    y <- data$y
    sums <- data$sums
    nums <- data$nums
    
    mu_hat <- kappa_hat + eta_hat*sums - eta_hat*nums*kappa_hat
    
    sum((y - mu_hat)^2)/length(y)
  }
 
  # data prep
  neigh_vals <- matrix(data[neighbors[[1]][, -1]], ncol = ncol(neighbors[[1]]) - 1)
  sums <- rowSums(neigh_vals)
  nums <- rowSums(!is.na(neigh_vals))
  data <- list(y = data, sums = sums, nums = nums)

  # get estimates
  params0_vec <- do.call(c, params0[c("kappa", "eta")])
  mple <- list()
  roots <- suppressWarnings(multiroot(score_fns, params0_vec, parms = data, maxiter = 2000))

  mple$eta <- roots$root[2]
  mple$kappa <- roots$root[1]
  mple$rho <- sqrt(rho2_hat(mple$kappa, mple$eta, data))
  
  return(mple)
}


# Computed p-values from test statistics
lognormal_dat <- exp(conclique_dat)

params0 <- list(rho = sd(lognormal_dat), kappa = mean(lognormal_dat), eta = 0)
mple_gauss <- fit_gauss_4nn(lognormal_dat, neighbors, params0)
gof_bs_gauss <- bootstrap_gof(lognormal_dat, concliques, neighbors, inits, "gaussian_single_param", "gaussian_single_param", "fit_gauss_4nn", params0, 5000, "ks", "max", plot.include = TRUE)

params0 <- list(rho = sd(log(lognormal_dat)), kappa = mean(log(lognormal_dat)), eta = 0)
mple_lognormal <- fit_gauss_4nn(log(lognormal_dat), neighbors, params0)
gof_bs_lognormal <- bootstrap_gof(log(lognormal_dat), concliques, neighbors, inits, "gaussian_single_param", "gaussian_single_param", "fit_gauss_4nn", params0, 5000, "ks", "max", plot.include = TRUE)



```

\begin{block}{Proposed \texttt{R} package can do these tests with concliques}
\begin{itemize}
\item Simulated one realization of lognormal conditionals on $20\times 20$:\\[.2cm]
  $\log Y(\boldsymbol s_i)$ given neighbors $\{ \boldsymbol s_i + (0,\pm 1), \boldsymbol s_i + (\pm 1, 0) \}$ is normal
  with variance $\tau^2$ and mean $\mu(\boldsymbol s_i) = \alpha + \eta \sum_{\boldsymbol s_j \in \mathcal{N}_i}[\log y(\boldsymbol s_j)-\alpha]$\\[.1cm]
\item Fit Gaussian MRF \& fit log Gaussian MRF to data $Y(\boldsymbol s_i)$ using pseudo-likelihood
\end{itemize}
\begin{table}
\footnotesize
\centering
\begin{tabular}{ccccc} \hline
      & Expected       & Conditional  &   & Model \\
Model  & Value $\alpha$ & Variance $\tau^2$ & Dependence $\eta$& $p-$value \\ \hline
True & $`r round(params$kappa, 2)`$ & $`r round(params$rho^2, 2)`$ & $`r round(params$eta, 2)`$ & \\
Log-Gaussian & $`r round(mple_lognormal$kappa, 2)`$ & $`r round(mple_lognormal$rho^2, 2)`$  & $`r round(mple_lognormal$eta, 2)`$ & $`r format(gof_bs_lognormal$p.value, 4)`$ \\
Gaussian & $`r round(mple_gauss$kappa, 2)`$ & $`r round(mple_gauss$rho^2, 2)`$  & $`r round(mple_gauss$eta, 2)`$ & $`r format(gof_bs_gauss$p.value, 4)`$ \\ \hline
\end{tabular}
\end{table}
\end{block}

# Reference distributions

```{r, fig.show='hold', fig.width=2.5, fig.height=2}
gof_bs_lognormal$plot
gof_bs_gauss$plot + xlim(c(2, 8)) + ylab("")
```

Bootstrapped reference distributions for the maximum across concliques of the Kolmogorov-Smirnov statistic from data generated from a four-nearest neighbor lognormal MRF with $\tau^2 = 2, \alpha = 10, \eta = 0.24$ and fit with a lognormal (left) and Gaussian (right) model.

# `conclique`

`R` package (to appear on CRAN) can be installed via GitHub using the following `R` code.

```{r, eval=FALSE, echo=TRUE}
devtools::install_github("andeek/conclique")
```

- Convenience functions `lattice_4nn_torus` and `min_conclique_cover`
\vspace{.1in}
- Gibbs samplers `run_conclique_gibbs` and `run_sequential_gibbs`
\vspace{.1in}
- GOF functions `spatial_residuals` and `gof_statistics`
\vspace{.1in}
- Bootstrap function `bootstrap_gof` 

# Extending `conclique`

One of the **key advantages** to using conclique-based approaches for simulation (and GOF tests) is the ability to consider non-Gaussian conditional models that go beyond a four-nearest neighbor structure.

`conclique` is generalizable in

- Dependence structure - beyond four-nearest neighbor
- Conditional distribution for each spatial location - beyond Gaussian and binary
- Generalized spatial residuals - for a user-supplied conditional distribution
- GOF statistics - aggregation beyond mean and max

# Perks 

**Geometric Ergodicity**

- Guaranteed convergence rate to the target joint data distribution for many (common) spatial MRF models
- With other established results, can obtain CLTs and Monte Carlo sample size assessments [@chan1994discussion; @jones2004markov; @hobert2002applicability; @roberts1997geometric]

**Speed & Flexibility**

- Computationally more efficient alternative to the standard (sequential) Gibbs sampler
- Same general applicability in allowing accessible simulation for a wide variety of MRFs
    * Not limited to any one model or family or models
    * Can be applied to irregular lattices and non-standard neighborhoods

# Future work and ideas

- Goodness-of-fit test for network data
    - The model-based method of resampling re-frames network into a collection of (Markovian) neighborhoods by using covariate information
    - Creates concliques on a graph structure
    - Use a conditionally specified network distribution (@casleton2017local) to sample network data in a blockwise conclique-based Gibbs sampler.
\vspace{.1in}
- Bootstrap theory for approximating GOF statistics is ongoing work
\vspace{.1in}
- More user friendly API for `conclique` to appear on CRAN

# Thank you


Questions?


\vfill

* Slides -- <http://bit.ly/kaplan-phd>
\vspace{.1in}
* Contact
    * Email -- <ajkaplan@iastate.edu>
    * Twitter -- <http://twitter.com/andeekaplan>
    * GitHub -- <http://github.com/andeek>


# References {.allowframebreaks} 

\tiny
