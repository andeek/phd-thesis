---
title: "A note on the instability and degeneracy of deep learning models"
shorttitle: "Instability"
author: Andee Kaplan
shortname: Kaplan, et al.
institute: |
    | Iowa State University
    | ajkaplan@iastate.edu
shortinstitute: ajkaplan@iastate.edu
date: |
  | June 22, 2017
  |
  | Slides available at <http://bit.ly/kaplan-private>
  |
  | \footnotesize Joint work with D. Nordman and S. Vardeman
shortdate: "June 22, 2017"
output: 
  beamer_presentation:
    keep_tex: false
    template: beamer.tex
    includes:
      in_header: front-matter.tex
theme: CambridgeUS
bibliography: [../resources/refs_rbm.bib]
fig_caption: true
nocite: |

---

```{r libraries, echo=FALSE, message=FALSE, warning=FALSE, cache=FALSE}
library(knitr)
library(ggplot2)
library(dplyr)
library(tidyr)
library(xtable)


set.seed(1022)
opts_chunk$set(echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE)
theme_set(theme_bw(base_family = "serif", base_size = 20))
```

# Introduction

- A probability model exhibits *instability* if small changes in a data outcome result in large changes in probability
- Model *degeneracy* implies placing all probability on a small portion of the sample space

\vfill

**Goal:** Quantify instability for a general and broad class of probability models defined on sequences of observations, where each sequence of length $N$ has a finite number of possible outcomes

\vfill

\begin{block}{Notation}
\begin{itemize}
\itemsep .2in
\item $\boldsymbol X = (X_1, \dots, X_N)$ a set of discrete random variables with a finite sample space, $\mathcal{X}^N$
\item For each $N$, $P_{\boldsymbol \theta_N}$ is a probability model on $\mathcal{X}^N$
\end{itemize}
\end{block}

# FSFS models

\begin{block}{Finitely Supported Finite Sequence (FSFS) model class}
A series $P_{\boldsymbol \theta_N}$ of probability models, indexed by a generic sequence of parameters $\boldsymbol \theta_N$, for describing data of length $N \geq 1$. The model support of $P_{\boldsymbol \theta_N}$ equals the (finite) sample space $\mathcal{X}^N$. 
\end{block}

\vfill

- The size and structure of such parameters $\boldsymbol \theta_N$ are without restriction 
\vspace{.2in}
- Natural cases include $\boldsymbol \theta_N \in \mathbb{R}^{q(N)}$ for some arbitrary integer-valued function $q(\cdot) \geq 1$

# Discrete exponential family models

Exponential family model for $\boldsymbol X$ with pmf of the form
$$
p_{N, \boldsymbol \lambda}(\boldsymbol x) = \exp\left[\boldsymbol\eta^T(\boldsymbol \lambda) \boldsymbol g_N(\boldsymbol x) - \psi(\boldsymbol \lambda)\right], \quad \boldsymbol x \in \mathcal{X}^N,
$$ 
with parameter $\boldsymbol \lambda \in \Lambda \subset \mathbb{R}^{k}$ and natural parameter function $\boldsymbol \eta : \mathbb{R}^k \mapsto \mathbb{R}^L$ spaces, $\boldsymbol g_N : \mathcal{X}^N \mapsto \mathbb{R}^L$ a vector of sufficient statistics, normalizing function
$$
\psi(\boldsymbol \lambda) = \log \sum\limits_{\boldsymbol x \in \mathcal{X}^N}\exp\left[\boldsymbol \eta^T(\boldsymbol \lambda) \boldsymbol g_N(\boldsymbol x) \right], \qquad \boldsymbol \lambda \in \Lambda,
$$
and $\Lambda = \{\boldsymbol \lambda \in \mathbb{R}^k : \psi(\boldsymbol \lambda) < \infty, k \le q(N) \}$ is the parameter space (fixed $k, L$ above). 


# Discrete exponential family models (cont'd)

- Such models arise with 
\vspace{.1in}
    - Spatial data on a lattice [@besag1974spatial]
\vspace{.1in}    
    - Network data [@wasserman1994social; @handcock2003assessing] 
\vspace{.1in}    
    - Standard independence models for discrete data ($N$ iid Bernoulli variables)
\vspace{.1in}    
- These models are special cases of the \blue{FSFS models}
\vspace{.1in}
- $P_{\boldsymbol \theta_N}(\boldsymbol x)\equiv p_{N,\boldsymbol \lambda_N}(\boldsymbol x)$ with $\boldsymbol \theta_N=\boldsymbol \lambda_N$ a sequence of elements of $\Lambda \subset \mathbb{R}^k$ 
\vspace{.1in}
- $P_{\boldsymbol \theta_N}(\boldsymbol x) > 0$ for all $\boldsymbol x \in \mathcal{X}^N$
\vspace{.1in}
- The dimension $k$ of the parameter $\boldsymbol \theta_N$ is the same for each $N$
\vspace{.1in}
- @schweinberger2011instability considered *instability* in such exponential models 


# Restricted Boltzmann machines

\begin{columns}[T] % align columns
\begin{column}{.5\textwidth}
\begin{figure}
  \centering
  \resizebox{\linewidth}{!}{\input{images/rbm.tikz}}
\end{figure}
Hidden nodes are indicated by gray filled circles and the visible nodes indicated by unfilled circles.
\vfill
\end{column}
\hfill
\begin{column}{.48\textwidth}
\begin{itemize}
\item $\mathcal{X} = \{-1,1\}$
\item $\boldsymbol X=(X_1,\ldots,X_N)$: $N$ random variables for visibles with support $\mathcal{X}^N$ 
\item $\boldsymbol H=(H_1,\ldots,H_{N_H})$: $N_H$ random variables for hiddens with support $\mathcal{X}^{N_H}$ 
\item Parameters $\boldsymbol \alpha \in \mathbb{R}^{N_H}$, $\boldsymbol \beta \in \mathbb{R}^N$, $\Gamma$ a matrix of size $N_H \times N$ ($\boldsymbol \theta_N = (\boldsymbol \alpha, \boldsymbol \beta,\Gamma) \in \Theta_N \subset \mathbb{R}^{q(N)}$ with $q(N) = N + N_H + N*N_H$)
\end{itemize}
\end{column}
\end{columns}

Joint pmf:
$$
P_{\boldsymbol \theta_N} (\tilde{\boldsymbol x}) = \exp\left[ \boldsymbol \alpha^T \boldsymbol h + \boldsymbol \beta^T \boldsymbol x + \boldsymbol h^T \Gamma \boldsymbol x - \psi(\boldsymbol \theta_N)\right], \quad \tilde{\boldsymbol x} = (\boldsymbol h, \boldsymbol x) \in \mathcal{X}^{N+N_H}
$$

# Restricted Boltzmann machines (cont'd)

- The pmf for the visible variables $X_1, \dots, X_N$ follows from marginalization:
    $$
    P_{\boldsymbol \theta_N} (\boldsymbol x) = \sum\limits_{\boldsymbol h \in \mathcal{X}^{N_H}} P_{\boldsymbol \theta_N} (\boldsymbol x, \boldsymbol h), \qquad \boldsymbol x \in \mathcal{X}^N.
    $$
- Size of $\boldsymbol \theta_N$, $q(N)$, increases as a function of sample dimension $N$
\vspace{.1in}
- Can choose the number $N_H$ of hidden variables to change with $N$ (potentially increase) 
\vspace{.1in}
- The RBM model specification for visibles is a \blue{FSFS model}
\vspace{.1in}
- Models formed by marginalizing a base FSFS model (e.g., a type of exponential family model) is again a \blue{FSFS model} class

# Deep learning

Two models with "deep architecture" that contain multiple hidden layers in addition to a visible layer of data

1. Deep Boltzmann machine (DBM)
    - Stacked RBMs with conditional dependence between neighboring layers.
    - The probability mass function for $X_1, \dots, X_N$ follows from marginalization of the joint pmf
2. Deep belief network (DBN)
    - **Similar** to a DBM: Multiple layers of latent random variables stacked in a deep architecture with no conditional dependence between layers 
    - **Difference**: all but the last stacked layer in a DBN are Bayesian networks [see @pearl985bayesian]
   

$q(N)$ is dependent on the dimension of the visibles  
$\Rightarrow$ visible DBM and DBN model specifications are both \blue{FSFS models}

# S-instability

\begin{block}{S-unstable FSFS models}
Let $\boldsymbol \theta_N \in \mathbb{R}^{q(N)}$ be a sequence of FSFS model parameters where the size of the model $q(N)$ is a function of the number of random variables $N$. A FSFS model formulation is \emph{Schweinberger-unstable} or \emph{S-unstable} if, as the number of variables increase ($N \rightarrow \infty$), 
\begin{align*}
\lim\limits_{N \rightarrow \infty} \frac{1}{N} \text{ELPR}(\boldsymbol \theta_N) \equiv \lim\limits_{N \rightarrow \infty} \frac{1}{N} \log \left[\frac{\max\limits_{(x_1, \dots, x_N) \in \mathcal{X}^N}P_{\boldsymbol \theta_N}(x_1, \dots, x_N)}{\min\limits_{(x_1, \dots, x_N) \in \mathcal{X}^N}P_{\boldsymbol \theta_N}(x_1, \dots, x_N)}\right] = \infty.
\end{align*}
\end{block}

This generalizes "unstable" from @schweinberger2011instability by allowing 

1. non-exponential family models and 
2. an increasing number of parameters

Differs in form but matches @schweinberger2011instability for exponential models

# Consequences of S-instability

Small changes in data can lead to overly-sensitive changes in probability. Let
$$
\Delta(\boldsymbol \theta_N) \equiv \max \left\{\log \frac{P_{\boldsymbol \theta_N}(\boldsymbol x)}{P_{\boldsymbol \theta_N}(\boldsymbol x^*)} : \boldsymbol x \text{ }\& \text{ } \boldsymbol x^* \in \mathcal{X}^N \text{ differ in exactly 1 component}\right\},
$$

\begin{block}{Proposition 1}
For an integer $N \ge 1$ and a given $C>0$, if $$\frac{1}{N}\text{ELPR}_N(\boldsymbol \theta_N) > C,$$ 
then 
$$\Delta_N(\boldsymbol \theta_N) > C.$$
\end{block}

\vfill

If the scaled ELPR is large, then the FSFS model can exhibit large changes in probability for small differences in the data configuration

# Tie to degeneracy

Define a $\epsilon$-modal set 
$$
M_{\epsilon, \boldsymbol \theta_N} \equiv \left\{\boldsymbol x \in \mathcal{X}^N: \log P_{\boldsymbol \theta_N}(\boldsymbol x) > (1-\epsilon)\max\limits_{\boldsymbol x^* \in \mathcal{X}^N}P_{\boldsymbol \theta_N}(\boldsymbol x^*) + \epsilon\min\limits_{\boldsymbol x^* \in \mathcal{X}^N}P_{\boldsymbol \theta_N}(\boldsymbol x^*) \right\}
$$
of possible outcomes, for a given $0 < \epsilon < 1$.

\begin{block}{Proposition 2}
For an S-unstable FSFS model and for any given $0 < \epsilon < 1$, 
$$
P_{\boldsymbol \theta_N}\left((x_1, \dots, x_N) \in M_{\epsilon, \boldsymbol \theta_N}\right) \rightarrow 1 \text{ as } N \rightarrow \infty.
$$
\end{block}

\vfill

In S-unstable FSFS models, all probability in the model formulation with a large number of random variables will concentrate mass on an $\epsilon$-mode set for any arbitrarily small $\epsilon$ (potentially small set of outcomes with most probability)

# Implications

For a large class of models, including "deep learning" models, we have 

\vspace{.2in}
1. Developed a formal definition of instability
\vspace{.2in}
2. Shown potential consequences of instability (degeneracy) 

\vfill

Models that fall within the definition of a FSFS model should be used with **caution** to ensure that the effects of instability are not experienced


# Thank you

Questions?


\vfill

* Slides -- <http://bit.ly/kaplan-private>
\vspace{.1in}
* Contact
    * Email -- <ajkaplan@iastate.edu>
    * Twitter -- <http://twitter.com/andeekaplan>
    * GitHub -- <http://github.com/andeek>


# References {.allowframebreaks} 

\tiny