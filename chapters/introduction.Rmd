# INTRODUCTION

This thesis is most generally focused on the development of statistical inference via Markov chain Monte Carlo (MCMC) techniques in complex data problems related to statistical learning, the analysis of network/graph data, and spatial resampling.

In particular, MCMC is a computational statistical approach for numerically approximating distributional quantities useful for inference that might otherwise be intractable to directly calculate. However, a challenge with MCMC methods is developing implementations which are both statistically rigorous and computationally scalable to large data sets. This work generally aims to bridge these aspects by exploiting conditional independence, or Markov structures, in data models. We investigate several problems in this context, such as (1) a statistical quantification of graph models used in deep machine learning and image classification and (2) the development of new, fast methods for simulating spatial, network, and other data with complex dependencies.

## Restricted Boltzmann machines

In recent years, restricted Boltzmann machines (RBMs) have risen to prominence due to their connection to deep learning. A RBM is an undirected graphical model with two layers, one hidden and one visible, used for describing data generation. By incorporating a hidden layer, RBMs are thought to have the ability to encode very complex and rich structures in data, making them attractive for supervised learning. However, the statistical properties of this model for conceptualizing data are largely unexplored in the literature, and the commonly cited fitting methodology remains heuristic-based and incorportate rough estimation. In Chapter \@ref(rbm-chapter), we provide steps toward a thorough understanding of the model and its behavior from the perspective of statistical theory and then explore the possibility of a rigorous fitting methodology via MCMC. We have found the RBM model class to be surprisingly deficient in two fundamental ways, which suggests that such models should be used with caution for inference. 

First, these models are readily unsatisfactory as a conceptualization of how data are generated. In particular, RBMs often fail to generate outcomes that resemble realistic data by only returning the data configuration with the highest probability. Additionally, RBMs also easily exhibit a kind of instability, whereby small differences in the data values lead to dramatically different probabilities for data. In practice, this is seen when a single pixel change in an image results in a wildly different classification. In Chapter \@ref(instab-chapter), ee consider the problem of quantifying instability for general probability models defined on sequences of observations, where each sequence of length $N$ has a finite number of possible outcomes. The results presented apply to large classes of models commonly used in random graphs, network analysis, and machine learning contexts (including the RBM model class).

Secondly, the fitting of RBM models is problematic. As the size of these models grows, both maximum likelihood and Bayesian methods of fitting quickly become intractable. But even when fitting is feasible, there are unique challenges due to the overparameterization of RBMs. In Chapter \@ref(rbm-chapter), three MCMC-based Bayesian modeling techniques are proposed and compared, each with the goal of avoiding parts of the RBM parameter space that yield improper models. With increased computational intensity comes an a potential improvement in fitting accuracy, but at the cost of feasibility. Additionally, because these RBM models are highly overparametrized, any principled fitting method simply reproduces the empirical distribution of most training sets unless the flexibility of the model is intentionally (and severely) limited. Hence, there is very little insight to be gained in fitting RBM models with sophisticated statistical methods. We conclude from this work that any model built using these structures, like a deep Boltzmann machine, is unlikely to achieve prediction or inference in a fully principled way without limiting the flexibility of the model class.

## Conclique-based Gibbs sampler

For spatial data (including graph, network, and other data structures), conditionally specified models can be formulated on the basis of an underlying Markov random field. This approach often provides an attractive alternative to direct specification of a full joint data distribution, which may be difficult for large, correlated data structures. Hence, the model is defined by prescribing a conditional distribution for each spatial location which depends on a set of neighboring observations. For such Markov random field models, we have developed a new and fast way to simulate data, which has provable MCMC accuracy (or convergence rate) properties, such as geometric ergodicity. Chapter \@ref(conc-chapter) presents this method and provides a real data example that showcases the flexibility and speed of the method. In addition, simulations are presented that show the algorithmic complexity to be comparable to the current standard method of simulating spatial data via Gibbs sampler. This work is important because it allows one to test statistical hypotheses or assess the goodness-of-fit models, by simulating data (e.g. spatial or network) to construct or approximate sampling distributions of statistics using a method which can be run in real time.

The simulation approach is based on using the Markov random field model structure to split data into groups of non-neighboring spatial observations, called concliques. Under a hypothesized Markov model structure, spatial residuals within each conclique are independent and identically distributed as uniform variables. This means that data can be generated with a MCMC technique (i.e., a Gibbs sampler) whereby observations within each conclique are simultaneously and independently updated.

We have formally established that this conclique-based Gibbs sampling method is geometrically ergodic [^1] for many four-nearest neighbor MRF models, including models for both continuous and discrete data, under minimal assumptions. This result holds because in the four-nearest neighbor structure, it is possible to partition the lattice into two conditionally independent concliques. Not only is this method provably geometrically ergodic in many cases, it is also computationally fast due to the ability to lower the number of steps necessary to run a single Gibbs iteration (the number of concliques vs. the number of observations).

[^1]: Geometric ergodicity in a Markov chain refers to the speed at which the chain converges to target distributions.

Chapter \@ref(package-chapter) presents a flexible `R` package (called `conclique`, to appear on CRAN) that implements a conclique-based Gibbs sampler while allowing the user to specify an arbitrary model. `conclique` is implemented in `Rcpp`, which improves the speed of the software but also allows for the user to specify all aspects of the model, including an arbitrary Markov dependence structure. By implementing the conclique-based Gibbs sampler in `R`, the methodology will have increased impact for the analysis of spatial and/or network data. 
