# INTRODUCTION

This thesis is most generally focused on the development of statistical inference via Markov chain Monte Carlo (MCMC) techniques in complex data problems related to statistical learning, the analysis of network/graph data, and spatial resampling.

In particular, MCMC is a computational statistical approach for numerically approximating distributional quantities useful for inference that might otherwise be intractable to directly calculate. However, a challenge with MCMC methods is developing implementations which are both statistically rigorous and computationally scalable to large data sets. This work generally aims to bridge these aspects by exploiting conditional independence, or Markov structures, in data models. We investigate several problems in this context, such as (1) a statistical quantification of graph models used in deep machine learning and image classification and (2) the development of new, fast methods for simulating spatial, network, and other data with complex dependencies.

## Restricted Boltzmann machines

In recent years, restricted Boltzmann machines (RBMs) have risen to prominence in the data mining and machine learning due to their connection to deep learning, specifically in stacked RBMs [see @salakhutdinov2009deep; @salakhutdinov2012efficient; @srivastava2013modeling; @le2008representational for examples]. A RBM is an undirected graphical model (for discrete or continuous random variables) with two layers, one hidden and one visible, used for describing data generation [@smolensky1986information]. By incorporating a hidden layer, RBMs are thought to have the ability to encode very complex and rich structures in data, making them attractive for supervised learning. However, the statistical properties of this model for conceptualizing data are largely unexplored in the literature, and the commonly cited fitting methodology remains heuristic-based [@hinton2006fast]. In Chapter \@ref(rbm-chapter), we provide steps toward a thorough understanding of the model and its behavior from the perspective of statistical theory and then explore the possibility of a rigorous fitting methodology via MCMC. We have found the RBM model class to be concerning in two fundamental ways, which suggests that such models should be used with caution for inference. 

First, these models are often unsatisfactory as a conceptualization of how data are generated. Recalling @fisher1922mathematical, the aim of a statistical model is to represent data in a compact way. However, RBMs often fail to generate outcomes that resemble realistic data by only returning the data configuration with the highest probability probability under the model. In other words, when sampling data from a degenerate RBM, only a small number of output possibilities have probability greater than zero, and thus a random sample of images may consist of several copies of the same one image (or small number of images).

In addition to degeneracy, RBMs also easily exhibit a type of instability, whereby small differences in the data values lead to dramatically different probabilities for data. In practice, this is seen when a single pixel change in an image results in a wildly different classification. Such model properties have recently been observed in RBMs [@li2014biclustering], as well as other deep architectures [@szegedy2013intriguing; @nguyen2014deep]. In Chapter \@ref(instab-chapter), we consider the problem of quantifying instability for general probability models defined on sequences of observations, where each sequence of length $N$ has a finite number of possible outcomes. The results presented apply to large classes of models commonly used in random graphs, network analysis, and machine learning contexts (including the RBM model class).

Beyond model properties of the RBM class, the fitting of RBM models is problematic. As the size of these models grows, both maximum likelihood and Bayesian methods of fitting quickly become intractable. But even when fitting is feasible, there are unique challenges due to the overparameterization of RBMs. In Chapter \@ref(rbm-chapter), three MCMC-based Bayesian modeling techniques are proposed and compared, each with the goal of avoiding parts of the RBM parameter space that yield improper models. With increased computational intensity comes an a potential improvement in fitting accuracy, but at the cost of feasibility. Additionally, because these RBM models are highly overparametrized, any principled fitting method will seek to reproduce the empirical distribution of most training sets. Hence, there is very little smoothing to be gained in fitting RBM models with sophisticated statistical methods. We conclude from this work that any model built using these structures, like a deep Boltzmann machine, is unlikely to achieve prediction or inference in a fully principled way without limiting the flexibility of the model class. This concern with overfitting is addressed via specification of a Bayesian prior to varying levels of success.

## Conclique-based Gibbs sampler

For spatial data (including graph, network, and other data structures), conditionally specified models can be usefully formulated on the basis of an underlying Markov random field [MRF; @besag1974spatial]. This approach often provides an attractive alternative to direct specification of a full joint data distribution, which may be difficult for large, correlated data structures (e.g. spatial data). Hence, the model is defined by prescribing a full conditional distribution for each spatial location which functionally depends on a set of observations with neighboring locations in the conditional model statement. Such Markov random field models have a natural and well-known connection to the Gibbs sampler through the conditionally specified distributions; see @besag1994discussion and @kaiser2000construction. The current Gibbs strategy for simulating from such MRF models involves single-site, or sequential, updating, whereby each observation in the field is simulated or updated individually [@besag1991bayesian].

Chapter \@ref(conc-chapter) develops a new and fast way to simulate data, which has provable MCMC accuracy (or convergence rate) properties, such as geometric ergodicity[^1]. As explained in Chapter \@ref(conc-chapter), the simulation approach involves a type of block, or group-wise, updating Gibbs sampler based on the notion of concliques, where the latter are sets of locations which are mutually non-neighboring (cf. [@kaiser2012goodness]). The simulation approach is based on using the Markov random field model structure to split data into groups of non-neighboring spatial observations, called concliques. Under a hypothesized Markov model structure, spatial residuals within each conclique are independent and identically distributed as uniform variables. This means that data can be generated with a MCMC technique (i.e., a Gibbs sampler) whereby observations within each conclique are simultaneously and independently updated. A real data example is first provided to initially highlight the flexibility and speed of the method. In addition, numerical studies show that the proposed sampler exhibits algorithmic, or mixing, performance resembling that of the current standard for simulating spatial data via the sequential Gibbs sampler. While mixing behavior may be similar, the proposed sampler is illustrated to have substantial advantages in computational speed or efficiency compared to the standard Gibbs approach. These findings are useful in that model-based simulation from MRF specifications plays an important role in statistical inference, particularly in generating or approximating reference distributions for spatial statistics which might otherwise be intractable to obtain. Since the latter can require generation of large collections of spatial datasets, the proposed conclique-based Gibbs sampler can be helpful to simulating and performing inference in a computationally manageable timeframe. 

[^1]: Geometric ergodicity in a Markov chain refers to the speed at which the chain converges to target distributions.

Chapter \@ref(conc-chapter) further formally established that the proposed conclique-based Gibbs sampling method is geometrically ergodic for many four-nearest neighbor MRF models, including models for both continuous and discrete spatial data, under minimal assumptions. Not only does this guarantee a particularly fast mixing rate for the Markov chain, but geometric ergodicity can be used with the results of @chan1994discussion to establish central limit theorems [@jones2004markov; @hobert2002applicability; @roberts1997geometric]. This result holds, because in the four-nearest neighbor structure, it is possible to partition the spatial locations into two concliques, where observations associated with one conclique are conditionally independent given the ibservations in the other conclique. Currently, the sequential Gibbs sampler for spatial datasets cannot be shown to be geometrically ergodic (for a data set of realistic size) because the state-of-the-art MCMC theory for proving geometric ergodicity is limited to less than 4-components in the Gibbs sampler; see, @johnson2015geometric; @hobert1998geometric; @tan2009block; @doss2010estimation; @jones2004sufficient; and @johnson2015geometricergodicity.

Chapter \@ref(package-chapter) then presents a flexible `R` [@r-lang] package (called `conclique`, to appear on CRAN) that implements a conclique-based Gibbs sampler while allowing the user to specify an arbitrary model. `conclique` is implemented in `Rcpp` [@rcpp], which improves the speed of the software but also remains flexible enought to allow for the user to specify all aspects of the model, including an arbitrary Markov dependence structure. By implementing the conclique-based Gibbs sampler in `R`, the methodology has greater potential impact for the analysis of spatial and/or network data. 
