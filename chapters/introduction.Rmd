# INTRODUCTION

My research has generally focused on developing statistical inference via Markov chain Monte Carlo (MCMC) techniques in complex data problems related to statistical learning, the analysis of network/graph data, and spatial resampling.

In particular, MCMC is a computational statistical approach for numerically approximating  distributional quantities useful for inference that might otherwise be intractable to directly calculate. However, a challenge with MCMC methods is developing implementations which are both statistically rigorous and computationally scalable to large data sets. My work generally aims to bridge these aspects by exploiting conditional independence, or Markov structures, in data models. My dissertation has investigated several problems in this context, such as (1) a statistical quantification of graph models used in deep machine learning and image classification and (2) the development of new, fast methods for simulating spatial, network, and other data with complex dependencies.

My research endeavors reflect a strong value for how research is conducted and results are shared. Specifically, I am a proponent of open source software and reproducibility in research. My work is open sourced, developed in the open online community, and is often accompanied by a software package or application. By espousing these practices, I have enjoyed collaborations with other researchers in the field with the same values, and our work has greatly benefited.

## Restricted Boltzmann machines

In recent years, restricted Boltzmann machines (RBMs) have risen to prominence due to their connection to deep learning. A RBM is an undirected graphical model with two layers, one hidden and one visible, used for describing data generation. By incorporating a hidden layer, RBMs are thought to have the ability to encode very complex and rich structures in data, making them attractive for supervised learning. However, the statistical properties of this model for conceptualizing data are largely unexplored in the literature, and the commonly cited fitting methodology remains heuristic-based and abstruse. In my dissertation work, I provide steps toward a thorough understanding of the model and its behavior from the perspective of statistical theory and then explore the possibility of a rigorous fitting methodology via MCMC. I have found the RBM model class to be surprisingly deficient in two fundamental ways, which suggests that such models should be used with caution for inference. 

First, these models are readily unsatisfactory as a conceptualization of how data are generated. In particular, RBMs often fail to generate outcomes that resemble realistic data by only returning the data configuration with the highest probability. Additionally, RBMs also easily exhibit a kind of instability in the parameter space, whereby small changes in parameters lead to dramatically different probabilities for data. In practice, this is seen when a single pixel change in an image results in a wildly different classification. 

Secondly, the fitting of these models is problematic. As the size of these models grows, both maximum likelihood and Bayesian methods of fitting quickly become intractable. But even when fitting is feasible, there are unique challenges due to the overparameterization of RBMs. I have proposed and compared three MCMC-based Bayesian modeling techniques, each with the goal of avoiding parts of the RBM parameter space that yield improper models. With increased computational intensity comes an a potential improvement in fitting accuracy, but at the cost of feasibility. Additionally, because these RBM models are highly overparametrized, any principled fitting method simply reproduces the empirical distribution of most training sets. Hence, there is very little insight to be gained in fitting RBM models with sophisticated statistical methods. I conclude from this work that any model built using these structures, like a deep Boltzmann machine, is unlikely to achieve prediction or inference in a fully principled way.

## Conclique-based Gibbs sampler

For spatial data (including graph, network, and other data structures), conditionally specified models can be formulated on the basis of an underlying Markov random field. This approach often provides an attractive alternative to direct specification of a full joint data distribution, which may be difficult for large, correlated data structures. Hence, the model is defined by prescribing a conditional distribution for each spatial location which depends on a set of neighboring observations. For such Markov random field models, I have developed a new and fast way to simulate data, which has provable MCMC accuracy (or convergence rate) properties, such as geometric ergodicity. This is important because it allows one to test statistical hypotheses or assess the goodness-of-fit models, by simulating data (e.g. spatial or network) to construct or approximate sampling distributions of statistics.

The simulation approach is based on using the Markov random field model structure to split data into groups of non-neighboring spatial observations, called concliques. Under a hypothesized Markov model structure, spatial residuals within each conclique turn out to be independent and identically distributed as uniform variables. This means that data can be generated with a MCMC technique (i.e., a Gibbs sampler) whereby observations within each conclique are simultaneously and independently updated.

I have formally established that this conclique-based Gibbs sampling method is geometrically ergodic [^1] for many four-nearest neighbor MRF models, including models for both continuous and discrete data, under minimal assumptions. This result holds because in the four-nearest neighbor structure, it is possible to partition the lattice into two concliques. I then use the independence of the conditional distributions of each data point in a conclique in conjunction with inverse sampling to construct a block update Gibbs sampler. Not only is this method provably geometrically ergodic in many cases, it is also computationally fast due to the ability to lower the number of steps necessary to run a single Gibbs iteration (the number of concliques vs. the number of observations).

[^1]: Geometric ergodicity in a Markov chain refers to the speed at which the chain converges to target distributions.

I have additionally written a flexible `R` package (called `conclique`, to appear on CRAN) that implements a conclique-based Gibbs sampler while allowing the user to specify an arbitrary model. By implementing the conclique-based Gibbs sampler in `R`, the methodology will have increased impact for the analysis of spatial and/or network data.
