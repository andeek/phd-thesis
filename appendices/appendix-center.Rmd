# (APPENDIX) Appendix {-}

# ON THE ONE-TO-ONE CORRESPONDENCE BETWEEN PARAMETERS AND MOMENTS IN THE RBM DISTRIBUTION {#appendix-rbm}

For integers $m,n \geq 1$, consider random vectors $\bm{V}_m \equiv(V_1,\ldots,V_m)$ and $\bm{H}_n\equiv (H_1,\ldots,H_n)$, where the random variables $V_i$ and $H_j$ assume values in $\{1,-1\}$, $i=1,\ldots,m$, $j=1,\ldots,n$. We suppose the vector $(\bm{V}_m,\bm{H}_n)$ follows a restricted Boltzmann machine (RBM) distribution, i.e. has probability mass function
\begin{align*}
&P(V_1= v_1,\ldots,V_m= v_m, H_1= h_1,\ldots,H_n=h_n|\bm{\theta}) \propto \exp\left[ \sum_{i=1}^m v_i \theta_{v_j} + \sum_{j=1}^n h_j \theta_{h_j} + \sum_{i=1}^m  \sum_{j=1}^n v_i  h_j \theta_{ij} \right], \\
& \quad \{v_i\}_{i=1}^m,\{h_j\}_{j=1}^n \subset\{\pm 1\},
\end{align*}
with $m+n+mn$ real parameters $\theta_{v_1},\ldots,\theta_{v_m},\theta_{h_1},\ldots,\theta_{h_m}$ and
$\theta_{ij}$, $i=1,\ldots,m$, $j=1,\ldots,n$ collected in a parameter vector denoted as $\bm{\theta}\in \mathbb{R}^{m+n+mn}$.
Denote the distribution of $(\bm{V}_m,\bm{H}_n)$ as RBM$(m,n,\bm{\theta})$. In the following, for convenience, we say a symmetric random variable $X$ has a Bernoulli$(1/2)$ distribution if $P(X = 1) = 1/2 = P(X = -1)$.

Use the notation $\bm{V}_m*\bm{H}_n = (V_1 H_1,\ldots,V_1 H_n, V_2 H_1,\ldots,V_2 H_n,\ldots, V_m H_1,\ldots,V_m H_n)$.
For the RBM$(m,n,\bm{\theta})$ model, consider in the function $\bm{g}: \mathbb{R}^{m+n+mn} \rightarrow   \mathcal{I} \subset \mathbb{R}^{m+n+mn}$ given by
\begin{align*}
\bm{g}(\bm{\theta}) &= \E_{\bm{\theta}}(\bm{V}_m,\bm{H}_n,\bm{V}_m*\bm{H}_n)\\
&= \sum_{\bm{v}_m \in \{\pm 1\}^m, \bm{h}_n \in \{\pm 1\}^n} (\bm{v}_m,\bm{h}_n,\bm{v}_m*\bm{h}_n)P(\bm{V}_m=\bm{v}_m,\bm{H}_n=\bm{h}_n|\bm{\theta})
\end{align*}
for $\bm{\theta}\in \mathbb{R}^{m+n+mn}$; above $\mathcal{I} \equiv \{\bm{g}(\bm{\theta}) : \bm{\theta}\in \mathbb{R}^{m+n+mn}\}$ denotes the image of the mapping.

We have the following properties:

- The function $\bm{g}: \mathbb{R}^{m+n+mn} \rightarrow \mathcal{I}$ is one-to-one; see Theorem \@ref(thm:one) below. Hence, these moments uniquely characterize the parameters in the RBM$(m,n,\bm{\theta})$ model.
- The image $\mathcal{I}$ is a symmetric set (i.e., $\bm{g}(\bm{\theta})=-\bm{g}(-\bm{\theta})$) and must be a connected (not necessarily convex) region in $\mathbb{R}^{m+n+mn}$, without any voids in it (by the continuity of $\bm{g}(\bm{\theta})$).
- The center of the image $\mathcal{I}$ (the zero vector in $\mathbb{R}^{m+n+mn}$) is given only by $\bm{\theta}=\bm{0}$ (the zero vector in the parameter space).  This corresponds to the case where all the random vectors $V_1,\ldots,V_m,H_1,\ldots,H_n$ are iid with a symmetric Bernoulli$(1/2)$ distribution; see Theorem \@ref(thm:two).

```{theorem, label="one", echo=TRUE}
Let $m,n\geq 1$ and suppose the random vector $(\bm{V}_m,\bm{H}_n) = (V_1,\ldots,V_m, H_1,\ldots,H_n)$ follows a RBM$(m,n,\bm{\theta})$ distribution. Let $\bm{\theta}_1,\bm{\theta}_2 \in \mathbb{R}^{m+n+mn}$ and
let $P_i$ and $\E$ denote probability and expectation under $\bm{\theta}_i$, $i=1,2$. Then, the following are equivalent

1. $\E_1 V_i=\E_2 V_i$, $\E_1 H_j=\E_2 H_j$ and $\E_1 V_i H_j =\E_2 V_i H_j$ for any $i=1,\ldots,m$, $j=1,\ldots,n$.
2. $\bm{\theta}_1 =\bm{\theta}_2$.
3. $P_1$ and  $P_2$ are the same distribution for  $(\bm{V}_m,\bm{H}_n)$.\\

```

```{proof, echo=TRUE}
We first establish the equivalence of claims 2 and 3. If claim 2 holds, then claim 3 holds trivially. Suppose claim 3 holds and pick $\ell \in \{1,\ldots,m\}$. Let  $\theta_{v_1},\ldots,\theta_{v_m},\theta_{h_1},\ldots,\theta_{h_n}$, $\theta_{ij}$, $i=1,\ldots,m$, $j=1,\ldots,n$, denote the components of $\bm{\theta}_1$ and let $\tilde{\theta}_{v_1},\ldots,\tilde{\theta}_{v_m},\tilde{\theta}_{h_1},\ldots,\tilde{\theta}_{h_n}$, $\tilde{\theta}_{ij}$, $i=1,\ldots,m$, $j=1,\ldots,n$, denote the analogous components of $\bm{\theta}_2$. Then, by assumption,
\begin{align*}
\exp\left[ 2\theta_{v_\ell} +2 \sum_{j=1}^n  h_j \theta_{\ell j} \right] &= \frac{P_1\left(\{V_\ell = 1\} \cap \bigcap_{i=1 \atop i \neq \ell}^m \{V_i=v_i\} \cap \bigcap_{j=1}^n \{H_j=h_j\}\right)} {P_1\left(\{V_\ell = -1\} \cap \bigcap_{i=1 \atop i \neq \ell}^m \{V_i=v_i\} \cap \bigcap_{j=1}^n \{H_j=h_j\}\right)}\\
&= \frac{P_2\left(\{V_\ell = 1\} \cap \bigcap_{i=1 \atop i \neq \ell}^m \{V_i=v_i\} \cap \bigcap_{j=1}^n \{H_j=h_j\}\right)}
{P_2\left(\{V_\ell = -1\} \cap \bigcap_{i=1 \atop i \neq \ell}^m \{V_i=v_i\} \cap \bigcap_{j=1}^n \{H_j=h_j\}\right)}\exp\left[ 2\tilde{\theta}_{v_\ell} +2 \sum_{j=1}^n  h_j \tilde{\theta}_{\ell j} \right]
\end{align*}
for any $\{v_i\}_{i=1,i\neq \ell}^m,\{h_j\}_{j=1}^n \subset\{\pm 1\}$. Equating exponents, we have $\theta_{v_\ell} - \tilde{\theta}_{v_\ell} = \sum_{j=1}^n  h_j (  \tilde{\theta}_{\ell j} -\theta_{\ell j})$ for any $\{h_j\}_{j=1}^n \subset \{\pm 1\}$. Taking all $h_j=1$ or all $h_j=-1$ for $1 \leq j \leq n$ yields $\theta_{v_\ell} = \tilde{\theta}_{v_\ell}$. This result further implies $\tilde{\theta}_{\ell 1} -\theta_{\ell 1} = -  \sum_{j=2}^n  h_j (  \tilde{\theta}_{\ell j} -\theta_{\ell j})$ for
any given $\{h_j\}_{j=2}^n \subset \{\pm 1\}$, so that $\theta_{\ell 1}=\tilde{\theta}_{\ell 1}$ must hold. Iterating this argument, sequentially for each $j=2,\ldots,m$, shows $\theta_{\ell j}=\tilde{\theta}_{\ell j}$ for any $1 \leq j \leq n$ in addition to $\theta_{v_\ell}=\tilde{\theta}_{v_\ell}$.  As $\ell\in\{1,\ldots,m\}$ was arbitrary, we have $\theta_{v_i}=\tilde{\theta}_{v_i}$ and $\theta_{i j}=\tilde{\theta}_{i j}$ for any $1 \leq i \leq m$, $1 \leq j \leq n$.

Now pick $k \in \{1,\ldots,n\}$ so that, by the RBM$(m,n,\bm{\theta})$ probability structure as above, we have analogously that
$$
\theta_{h_k} + \sum_{i=1}^m  v_i \theta_{i k} =  \tilde{\theta}_{h_k} + \sum_{i=1}^m  v_i \tilde{\theta}_{i k}
$$
for any $\{v_i\}_{i=1}^m \subset \{\pm 1\}$, so that $\theta_{h_k}  =  \tilde{\theta}_{h_k}$. As $k \in \{1,\ldots,n\}$ was arbitrary, we have now established claim 2 from claim 3. 

To establish the equivalence of claims 1 and 3 in Theorem \@ref(thm:one) under the RBM$(m,n,\bm{\theta})$ model, we note that claim 3 easily implies claim 1. Hence, it suffices now to establish claim 3 from claim 1. This follows immediately from Lemma \@ref(lem:first) below and the fact that the variables $V_1,\ldots,V_m$ are conditionally independent given $H_1,\ldots,H_n$ in the RBM$(m,n,\bm{\theta})$ model (and likewise $H_1,\ldots,H_n$ are independent given $V_1,\ldots,V_m$).
```

```{lemma, label="first", echo=TRUE}
Let $m,n\geq 1$. Let $P_1,P_2$ denote two probability distributions for a random vector $(\bm{V}_m,\bm{H}_n) = (V_1,\ldots,V_m, H_1,\ldots,H_n)$ supported on $\{a,b\}^{m+n}$, for some real constants $a\neq b \in\mathbb{R}$, such that $P_i( \bm{V}_m=\bm{v}_m, \bm{H}_n=\bm{h}_n)>0$, $i=1,2$, for any $\bm{v}_m \in \{a,b\}^{m}$, $\bm{h}_n \in \{a,b\}^{n}$. Suppose additionally that, under $P_1$ or $P_2$,  the variables $V_1,\ldots,V_m$ are conditionally independent given $\bm{H}_n=\bm{h}_n \in \{a,b\}^{n}$ and that the variables $H_1,\ldots,H_n$ are conditionally independent given $\bm{V}_m=\bm{v}_m \in \{a,b\}^{m}$.

Let $\E_i$ denote expectation under the distribution $P_i$, $i=1,2$. Then, for any $\bm{v}_m \in \{a,b\}^{m}, \bm{h}_n \in \{a,b\}^{n}$,
$$
P_1( \bm{V}_m=\bm{v}_m, \bm{H}_n=\bm{h}_n) = P_2( \bm{V}_m=\bm{v}_m, \bm{H}_n=\bm{h}_n), \qquad
$$
follows if $\E_1 V_i = \E_2 V_i$, $\E_1 H_j = \E_2 H_j$, $\E_1 V_i H_j = \E_2 V_j H_j$ holds for all $i\in\{1,\ldots,m\}$, $j\in \{1,\ldots,n\}$.
```

```{proof, echo=TRUE}
The case $m=n=1$ follows from Lemma \@ref(lem:second). We now use an induction argument, assuming that the result of Lemma \@ref(lem:first) holds for some given order of $(m,n)$ with $m,n\geq 1$ and show the the result continues to hold for $(m+1,n)$ or $(m,n+1)$. We shall treat the case $(m+1,n)$ (where the other case $(m,n+1)$ follows by symmetrical arguments).

Considering random variables $(\bm{V}_{m+1},\bm{H}_n) = (V_1,\ldots,V_m,V_{m+1}, H_1,\ldots,H_n)$, the induction hypothesis applies to $(\bm{V}_{m},\bm{H}_n)$ where $\bm{V}_m =(V_1,\ldots,V_m)$. Hence, for any $\bm{v}_m \in \{a,b\}^{m}, \bm{h}_n \in \{a,b\}^{n}$, we have $P_1(\bm{V}_m=\bm{v}_m,\bm{H}_n=\bm{h}_n) = P_2(\bm{V}_m=\bm{v}_m,\bm{H}_n=\bm{h}_n)>0$, $P_1( \bm{H}_n=\bm{h}_n) = P_2(\bm{H}_n=\bm{h}_n)>0$ and consequently
\begin{align}(\#eq:0)
P_1(\bm{V}_m=\bm{v}_m|\bm{H}_n=\bm{h}_n) =P_2(\bm{V}_m=\bm{v}_m |\bm{H}_n=\bm{h}_n).
\end{align}
Likewise, by the induction hypothesis applied to $(\bm{V}_{m-},\bm{H}_n)$ for $\bm{V}_{m-} = (V_2,\ldots,V_{m+1})$, we have
\begin{eqnarray}(\#eq:0)
 1- P_1(V_{m+1} = b | \bm{H}_n=\bm{h}_n) &=& P_1(V_{m+1} = a |\bm{H}_n=\bm{h}_n)\\
\nonumber  &=&  P_2(V_{m+1} = a | \bm{H}_n=\bm{h}_n)\\
\nonumber  & = &1-P_2(V_{m+1} = b | \bm{H}_n=\bm{h}_n)
\end{eqnarray}
for any $\bm{h}_n \in \{a,b\}^{n}$. Now fix  $\bm{v}_m \in \{a,b\}^{m}, \bm{h}_n \in \{a,b\}^{n}$ and let $\bm{v}_{m+1}= (v_{m+1},\bm{v}_m)$ for $v_{m+1}\in\{a,b\}$.  Then, by the conditional independence assumption,
$$
P_i( \bm{V}_{m+1}=\bm{v}_{m+1}| \bm{H}_n=\bm{h}_n) = P_i( V_{m+1}=v_{m+1}| \bm{H}_n=\bm{h}_n) P_i( \bm{V}_{m}=v_{m}| \bm{H}_n=\bm{h}_n)
$$
for $i=1,2$,  so that we have $P_1( \bm{V}_{m+1}=\bm{v}_{m+1}| \bm{H}_n=\bm{h}_n) = P_2( \bm{V}_{m+1}=\bm{v}_{m+1}| \bm{H}_n=\bm{h}_n)$ by (\@ref{eq:0})-(\@ref(eq:1)). Consequently, it follows that
\begin{eqnarray*}
P_1( \bm{V}_{m+1}=\bm{v}_{m+1}, \bm{H}_n=\bm{h}_n) &=& P_1( \bm{V}_{m+1}=\bm{v}_{m+1}| \bm{H}_n=\bm{h}_n) P_1(\bm{H}_n=\bm{h}_n)\\
& =& P_2( \bm{V}_{m+1}=\bm{v}_{m+1}| \bm{H}_n=\bm{h}_n)P_2(\bm{H}_n=\bm{h}_n) \\&=&P_2( \bm{V}_{m+1}=\bm{v}_{m+1}, \bm{H}_n=\bm{h}_n)
\end{eqnarray*}
by $P_1(\bm{H}_n=\bm{h}_n)=P_2(\bm{H}_n=\bm{h}_n)$. As $\bm{v}_{m+1}\in \{a,b\}^{m+1},\bm{h}_n \in \{a,b\}^{n}$ were arbitrary, this completes the proof of Lemma \@ref(lem:first). 
```

```{lemma, label="second", echo=TRUE}
Suppose the discrete random vector $(X,Y)$ has support $\{a,b\}\times \{a,b\}$, for some $a\neq b \in \mathbb{R}$,
under two probability distributions $P_1$ and $P_2$. Let $\E_i$ denote expectation under the distribution $P_i$, $i=1,2$. Then, the following are equivalent

1. $\E_1 X = \E_2 X$, $\E_1 Y = \E_2 Y$, $\E_1 X Y = \E_2 X Y$, where $\E_i$ denotes expectation under $P_i$, $i=1,2$.
2. $P_1(X=x,Y=y)=P_2(X=x,Y=y)$ for $x,y\in\{a,b\}$, i.e., $(X,Y)$ has the same distribution under $P_1$ and $P_2$.

```

```{proof, echo=TRUE}
Denote the four probabilities by which the random vector $(X,Y)$ assume pairs $(b,b),(b,a),(a,b),(a,a)$, respectively, as $c_1,c_2,c_3,1-c_1-c_2-c_3$ under $P_1$ and $d_1,d_2,d_3,1-d_1-d_2-d_3$ under $P_2$. The conditions $\E_1 X = \E_2 X$, $\E_1 Y = \E_2 Y$, $\E_1 X Y = \E_2 X Y$ with $b\neq a$ imply that
$$
(c_1+c_2) = (d_1+d_2), \quad  (c_1+c_3) = (d_1+d_3) \quad (b-a) c_1 + a (2c_1+c_2+c_3) =  (b-a) d_1 + a (2d_1+d_2+d_3).
$$
As $(2c_1+c_2+c_3) = (2d_1+d_2+d_3)$ and $b\neq a$, we conclude that $c_1=d_1$, from which it follows that $c_2=d_2$ and $c_3=d_3$. Hence, claim 1 implies claim 2 in Lemma \@ref(lem:second). Claim 2 also trivially implies claim 1. 
```
 
```{theorem, label="two", echo=TRUE}
Let $m,n\geq 1$ and suppose the random vector $(\bm{V}_m,\bm{H}_n) = (V_1,\ldots,V_m, H_1,\ldots,H_n)$ follows a RBM$(m,n,\bm{\theta})$ distribution.  Then, the following are equivalent

1. $\E V_i=0$, $\E H_j=0$ and $\E V_i H_j =0$ for any $i=1,\ldots,m$, $j=1,\ldots,n$.
2. $\bm{\theta} = \bm{0}\in \mathbb{R}^{m+n+mn}$.
3. $V_1,\ldots,V_m, H_1,\ldots,H_n$ are iid Bernoulli$(1/2)$ random variables.

```

```{proof, echo=TRUE}
We first establish the equivalence of claims 2 and 3. If claim 2 holds, then claim 3 follows easily from the resulting uniform cell probabilities: $P(V_1= v_1,\ldots,V_m= v_m, H_1= h_1,\ldots,H_n=h_n|\bm{\theta})=2^{-mn}$ for any $v_i,h_j\in\{\pm 1\}$, $i=1,\ldots,m$, $j=1,\ldots,n$.

Now suppose claim 3 holds. Pick $\ell \in \{1,\ldots,m\}$. Then, by the RBM$(m,n,\bm{\theta})$ probability structure and the iid Bernoulli assumption, we have the conditional probability
\begin{eqnarray*}
\frac{1}{2} &=& P\left(V_\ell = v_{\ell}\Big| \bigcap_{i=1 \atop i \neq \ell}^m \{V_i=v_i\} \cap \bigcap_{j=1}^n \{H_j=h_j\}\right) \\
&\equiv &\frac{\exp\left( v_\ell\theta_\ell +  v_\ell\sum_{j=1}^n  h_j \theta_{\ell j}\right)}{\exp\left(
\theta_\ell + \sum_{j=1}^n   h_j\theta_{\ell j} \right) +\exp\left(-\theta_\ell  -\sum_{j=1}^n  h_j\theta_{\ell j}\right) }
\end{eqnarray*}
for any subsets $\{v_i\}_{i=1}^m,\{h_j\}_{j=1}^n \subset\{\pm 1\}$. Equating conditional probabilities when $v_\ell=1$ or when $v_\ell=-1$, we have   $\theta_\ell = -  \sum_{j=1}^m  h_j \theta_{\ell j}$ for any given $\{h_j\}_{j=1}^n \subset \{\pm 1\}$. Taking all $h_j=1$ or all $h_j=-1$ for $1 \leq j \leq n$ yields $\theta_\ell=0$.  This result further implies $\theta_{\ell 1} = - \sum_{j=2}^n  h_j \theta_{\ell j}$ for any given $\{h_j\}_{j=2}^n \subset \{\pm 1\}$, so that $\theta_{\ell 1}=0$ must hold. Iterating this argument, sequentially for each $j=2,\ldots,m$, shows $\theta_{\ell j}=0$ for any $1 \leq j \leq n$ in addition to $\theta_{v_\ell}=0$. As $\ell\in\{1,\ldots,m\}$ was arbitrary, we have $\theta_{v_i}=0$ and $\theta_{i j}=0$ for any $1 \leq i \leq m$, $1 \leq j \leq n$.

Now pick $k \in \{1,\ldots,n\}$ so that, by the RBM$(m,n,\bm{\theta})$ probability structure and the iid assumption, we have the conditional probability
$$
 \frac{1}{2} = P\left(H_k = h_k \Big| \bigcap_{j=1 \atop j \neq k}^n \{H_j=h_j\}\right) \equiv \frac{\exp(h_k \theta_{h_k})}{\exp(\theta_{h_k}) + \exp(-\theta_{h_k})}
$$
for any $\{h_j\}_{j=1}^n \subset \{\pm 1\}$.  Considering $h_k=1$ or $h_k=-1$, we conclude that $\theta_{h_k} = -\theta_{h_k}$
or $\theta_{h_k}=0$.  As $k \in \{1,\ldots,n\}$ was arbitrary, we have now established claim 2 from claim 3.

To establish the equivalence of claims 1 and 2 in Theorem \@ref(thm:two) under the RBM$(m,n,\bm{\theta})$ model, we note that claim 2 again implies claim 3, which then easily implies claim 1. By Theorem \@ref(thm:one), if claim 1 holds then the only possibility for this is $\bm{\theta} = \bm{0} \in \mathbb{R}^{m+n+mn}$. 
```

# FLEXIBILITY OF THE RBM MODEL {#appendix-rbm2}

In an RBM model with enough hidden variables, parameter values may be chosen to match any given cell probabilities with arbitrary closeness. Additionally, when one or more of the cell probabilities (\@ref(eq:0)) are zero, the corresponding RBM probabilities may never be identically zero (due to exponential terms in the model) but parameters can be still selected to make the appropriate RBM cell probabilities arbitrarily small. We show this for a model with two visible variables $(V_1,V_2)$ and one hidden $H_1$.

To demonstrate, we assume $p_{(-1,-1)}>0$ (without loss of generality) in the specified cell probabilities (\@ref(eq:0)) and replace parameters $\theta_{11},\theta_{21}$ with $\Delta_1 \equiv \theta_{11} +\theta_{21}$ and $\Delta_2 \equiv \theta_{11} -\theta_{21}$.  We may then prescribe values of $\theta_{v_1}, \theta_{v_2}, \theta_{h_1}, \Delta_1,\Delta_2$  so that the model probability ratio
$$
P(V_1=v_1,V_2=v_2|\theta_{v_1}, \theta_{v_2}, \theta_{h_1}, \Delta_1,\Delta_2)/  P(V_1=-1,V_2=-1| \theta_{v_1}, \theta_{v_2}, \theta_{h_1}, \Delta_1,\Delta_2)
$$
matches the corresponding ratio $p_{(v_1,v_2)}/p_{(-1,-1)}$ over three values of $(v_1,v_2) = (1,-1),(-1,1),(1,1)$.

For instance, assuming the cell probabilities from (\@ref(eq:0)) are all positive, these probabilities can be exactly reproduced by choosing
\begin{eqnarray*}
\theta_{v_1} &=& \frac{1}{2} \log\left( \frac{p_{(1,-1)}}{p_{(-1,-1)}} \frac{\exp(\theta_{h_1}-\Delta_1) + \exp(-\theta_{h_1}+\Delta_1)}{\exp(\theta_{h_1}+\Delta_2) + \exp(-\theta_{h_1}-\Delta_2)}  \right),\\ 
\theta_{v_2}&= &\frac{1}{2} \log\left( \frac{p_{(-1,1)}}{p_{(-1,-1)}} \frac{\exp(\theta_{h_1}-\Delta_1) + \exp(-\theta_{h_1}+\Delta_1)}{\exp(\theta_{h_1}-\Delta_2) + \exp(-\theta_{h_1}+\Delta_2)}  \right)
\end{eqnarray*}
and selecting $\theta_{h_1}, \Delta_1,\Delta_2$ to solve
\begin{equation}
(\#eq:1)
\frac{p_{(1,1)}p_{(-1,-1)}}{p_{(-1,1)}p_{(1,-1)}} = \frac{\ell(|\theta_{h_1}|) + \ell(|\Delta_1|) }{\ell(|\theta_{h_1}|) + \ell(|\Delta_2|)},
\end{equation}
based on a monotonically increasing function $\ell(x)\equiv \exp(-2x)+\exp(2x)$, $x \geq 0$. If  $[p_{(1,1)}p_{(-1,-1)}]/ [p_{(-1,1)}p_{(1,-1)}] \geq 1$, one can pick any values for $\theta_{h_1}, \Delta_2\in \mathbb{R}$ and solve (\@ref(eq:1)) for $|\Delta_1|$; likewise, when $[p_{(1,1)}p_{(-1,-1)}]/ [p_{(-1,1)}p_{(1,-1)}] < 1$ in (\@ref(eq:1))}), one may solve for $|\Delta_2|$ upon choosing any values for $\theta_{h_1}, \Delta_1\in \mathbb{R}$. 
  
Alternatively, if exactly one specified cell probability in (\@ref(eq:0)) is zero, say $p_{(1,1)}$ (without loss of generality), we can select parameters $\theta_{v_1},\theta_{v_2}$ as above based on a sequence $(\theta_{h_1}, \Delta_{1}, \Delta_{2}) \equiv (\theta_{h_1}^{(m)}, \Delta_{1}^{(m)}, \Delta_{2}^{(m)})$, $m\in\{1,2,\ldots,\}$ of the remaining parameter values such that $\lim_{m\to \infty}|\Delta_{1}^{(m)}| = \infty$ and $\lim_{m\to \infty}  (|\theta_{h_1}^{(m)}| + |\Delta_{2}^{(m)}|)/|\Delta_{1}^{(m)}|=0$ hold. This guarantees that the resulting RBM model matches the given cell probabilities (\@ref(eq:0)) in the limit:
\begin{equation}
(\#eq:2)
\lim_{m\to \infty}P(V_1=v_1,V_2=v_2| \theta_{v_1}, \theta_{v_2}, \theta_{h_1},\Delta_1,\Delta_2) = p_{(v_1,v_2)},\quad (v_1,v_2)\in\{(\pm 1,\pm 1)\}.
\end{equation}  
If exactly two specified probabilities in (\@ref(eq:0)) are zero, say  $p_{(1,1)}$ and $p_{(-1,1)}$ (without loss of generality), then a limit approximation as in (\@ref(eq:2)) follows by picking $\theta_{v_1}$ as above based on any choices of $(\theta_{h_1}, \Delta_1,\Delta_2)$ and choosing  a sequence of $\theta_{v_2}\equiv \theta_{v_2}^{(m)}$ values for which $\theta_{v_2}^{({m})} \rightarrow -\infty$.