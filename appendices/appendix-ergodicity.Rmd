# PROOFS OF GEOMETRIC ERGODICITY FOR CONCLIQUE-BASED GIBBS SAMPLERS {#appendix-ergodicity}

\begin{lemma}
\label{lemma:invariant}
Assuming that the full conditionals for the MRF model specify a valid joint distribution, all three conclique-based Gibbs samplers (CGS, RQGS, and RSGS) yield the joint distribution $\Pi(\cdot)$ of $(Y(\boldsymbol s_1), \dots, Y(\boldsymbol s_n))$ as the invariant distribution.
\end{lemma}

\begin{proof}
Let $\boldsymbol y = (\boldsymbol y_1, \dots, \boldsymbol y_Q)$ and  $\boldsymbol x = (\boldsymbol x_1, \dots, \boldsymbol x_Q)$ with $\boldsymbol x_i, \boldsymbol y_i$ denoting potential values for the variables in conclique $i=1, \dots, Q$, with $\boldsymbol x_i, \boldsymbol y_i \in \mathbb{R}^{n_i}$ for integers $l_1, \dots, l_Q$. Let $l(j) = \sum_{i = j}^Q l_i$. Let $\pi(\boldsymbol x)$ denote the joint density of $\Pi(\cdot)$ with respect to a dominating measure $\mu$. Then, the one-step transition kernel in the Gibbs sampler has a density $k_{\sigma}(\boldsymbol y, \boldsymbol x), \sigma \in \{\text{CGS, RQGS, RSGS}\}$ as specified in Table \ref{tab:densities} with respect to the dominating measure. Pick or fix $\boldsymbol x \in \mathcal{X} \subset \mathbb{R}^n$ in the joint support of $\pi(\cdot)$ (i.e. $\pi(\boldsymbol x) > 0$). Let $S$ be a nonempty subset of $\{1, 2, \cdots, Q\}$ and, for $\boldsymbol x = (\boldsymbol x_1, \dots, \boldsymbol x_Q)$ as above, let $\boldsymbol x_S = \{\boldsymbol x_i: 1 \le i \le Q, i \in S\}$ and $\boldsymbol x_{-S} = \{\boldsymbol x_i: 1 \le i \le Q, i \not\in S\}$. For $i = 1, \dots Q$, write $\pi(\boldsymbol x_i|\cdot)$ to denote the conditional density for conclique $i$ values given values $(\cdot)$ of the other conclique observations and write $\pi(\boldsymbol x_S)$ to denote the marginal density f observations belonging to a conclique indexed by $S \subset \{1, \dots, Q\}$.

Then, for the CGS strategy, recall the transition density $k_{CGS}(\boldsymbol y, \boldsymbol x) = \prod_{i = 1}^Q\pi(\boldsymbol x_i|\boldsymbol x_{(1, \dots, i-1)}, \boldsymbol y_{-(1, \dots, i)})$  (where for $i = 1$, we have $\pi(\boldsymbol x_1|\boldsymbol y_{-1})$ notationally as well as $\pi(\boldsymbol x_Q|\boldsymbol x_{(1, \dots, Q-1)} = \pi(\boldsymbol x_q|\boldsymbol x_{-Q})$ for $i = Q$). We have

\begin{align*}
\int&\pi(\boldsymbol y) k_{CGS}(\boldsymbol y, \boldsymbol x) d\mu(\boldsymbol y)\\ 
&= \int\limits_{\mathbb{R}^{l(1)}} \pi(\boldsymbol y) \pi(\boldsymbol x_1|\boldsymbol y_{-1})\pi(\boldsymbol x_2 | \boldsymbol x_1, \boldsymbol y_{-(1,2)}) \cdots \pi(\boldsymbol x_Q|\boldsymbol x_{-Q})d\mu(\boldsymbol y) \\
&=  \pi(\boldsymbol x_Q|\boldsymbol x_{-Q})\int\limits_{\mathbb{R}^{l(1)}} \pi(\boldsymbol y) \pi(\boldsymbol x_1|\boldsymbol y_{-1})\pi(\boldsymbol x_2 | \boldsymbol x_1, \boldsymbol y_{-(1,2)}) \cdots \pi(\boldsymbol x_{Q-1}|\boldsymbol x_{-(Q-1, Q)}, \boldsymbol y_Q)d\mu(\boldsymbol y) \\
&=  \pi(\boldsymbol x_Q|\boldsymbol x_{-Q})\int\limits_{\mathbb{R}^{l(2)}} \pi(\boldsymbol y_{-1}) \pi(\boldsymbol x_1|\boldsymbol y_{-1})\pi(\boldsymbol x_2 | \boldsymbol x_1, \boldsymbol y_{-(1,2)}) \cdots \pi(\boldsymbol x_{Q-1}|\boldsymbol x_{-(Q-1, Q)}, \boldsymbol y_Q)d\mu(\boldsymbol y_{-1})\\
&=  \pi(\boldsymbol x_Q|\boldsymbol x_{-Q})\int\limits_{\mathbb{R}^{l(2)}} \pi(\boldsymbol y_{-1}, \boldsymbol x_1) \pi(\boldsymbol x_2 | \boldsymbol x_1, \boldsymbol y_{-(1,2)}) \cdots \pi(\boldsymbol x_{Q-1}|\boldsymbol x_{-(Q-1, Q)}, \boldsymbol y_Q)d\mu(\boldsymbol y_{-1}) \\
&=  \pi(\boldsymbol x_Q|\boldsymbol x_{-Q})\int\limits_{\mathbb{R}^{l(3)}} \pi(\boldsymbol y_{-(1,2)}, \boldsymbol x_1) \pi(\boldsymbol x_2 | \boldsymbol x_1, \boldsymbol y_{-(1,2)}) \cdots \pi(\boldsymbol x_{Q-1}|\boldsymbol x_{-(Q-1, Q)}, \boldsymbol y_Q)d\mu(\boldsymbol y_{-(1,2)})\\
& \quad \vdots \\
&= \pi(\boldsymbol x_Q|\boldsymbol x_{-Q}) \int\limits_{\mathbb{R}^{l(Q)}} \pi(\boldsymbol y_Q, \boldsymbol x_{-Q}) d\mu(\boldsymbol y_Q) \\
&= \pi(\boldsymbol x_Q|\boldsymbol x_{-Q})\pi(\boldsymbol x_{-Q}) \\
&= \pi(\boldsymbol x),
\end{align*}
establishing the result. Note that we technically assumed $\mathbb{R}^n = \mathcal{X}$, (i.e. $\pi(\boldsymbol y) > 0$ for $\boldsymbol y \in \mathbb{R}^n$) above for simplicity, but without loss of generality (as the same follows by partitioning $\mathbb{R}^n = \mathcal{X}\cup\mathbb{R}^n\setminus\mathcal{X}$ if necessary and partitioning $\mathbb{R}^{l(j)}$ along $\{\boldsymbol x_{-(1, \dots, j-1)}:\pi(\boldsymbol x_{-(1, \dots, j-1)}) > 0\}$ for $j = 1, \dots, Q$ generally). 

Likewise, for the RQGS strategy, it holds that
\begin{align*}
&\int\pi(\boldsymbol y) k_{RQGS}(\boldsymbol y, \boldsymbol x) d\mu(\boldsymbol y)\\ 
&= \int\limits_{\mathbb{R}^{l(1)}} \pi(\boldsymbol y) \sum\limits_{i = 1}^{Q!} q_i \pi(\boldsymbol x_{i(1)}|\boldsymbol y_{-i(1)})\pi(\boldsymbol x_{i(2)} | \boldsymbol x_{i(1)}, \boldsymbol y_{-(i(1), i(2))}) \cdots \pi(\boldsymbol x_{i(Q)}|\boldsymbol x_{-i(Q)})d\mu(\boldsymbol y) \\
&= \sum\limits_{i = 1}^{Q!} q_i \pi(\boldsymbol x_{i(Q)}|\boldsymbol x_{-i(Q)}) \int\limits_{\mathbb{R}^{l(1)}} \pi(\boldsymbol y) \pi(\boldsymbol x_{i(1)}|\boldsymbol y_{-i(1)})\pi(\boldsymbol x_{i(2)} | \boldsymbol x_{i(1)}, \boldsymbol y_{-(i(1), i(2))}) \cdots \pi(\boldsymbol x_{i(Q-1)}|\boldsymbol x_{-(i(Q-1), i(Q))}, \boldsymbol y_{i(Q)})d\mu(\boldsymbol y) \\
&= \sum\limits_{i = 1}^{Q!} q_i \pi(\boldsymbol x_{i(Q)}|\boldsymbol x_{-i(Q)}) \int\limits_{\mathbb{R}^{l(2)}} \pi(\boldsymbol y_{-i(1)}) \pi(\boldsymbol x_{i(1)}|\boldsymbol y_{-i(1)})\pi(\boldsymbol x_{i(2)} | \boldsymbol x_{i(1)}, \boldsymbol y_{-(i(1), i(2))}) \cdots \pi(\boldsymbol x_{i(Q-1)}|\boldsymbol x_{-(i(Q-1), i(Q))}, \boldsymbol y_{i(Q)})d\mu(\boldsymbol y_{-i(1)}) \\
& \quad \vdots \\
&= \sum\limits_{i = 1}^{Q!} q_i \pi(\boldsymbol x_{i(Q)}|\boldsymbol x_{-i(Q)})\int\limits_{\mathbb{R}^{l_{i(Q)}}} \pi(\boldsymbol y_{i(Q)}, \boldsymbol x_{-i(Q)}) d\mu(\boldsymbol y_{i(Q)}) \\
&= \sum\limits_{i = 1}^{Q!} q_i \pi(\boldsymbol x_{i(Q)}|\boldsymbol x_{-i(Q)})\pi( \boldsymbol x_{-i(Q)})\\
&= \pi(\boldsymbol x)\sum\limits_{i = 1}^{Q!}q_i \\
&= \pi(\boldsymbol x),
\end{align*}

while for the RSGS strategy, by marginalizing over $\boldsymbol y_i$ for each $i = 1, \dots, Q$,
\begin{align*}
\int\pi(\boldsymbol y) k_{RQGS}(\boldsymbol y, \boldsymbol x) d\mu(\boldsymbol y) 
&= \int\limits_{\mathbb{R}^{l(1)}} \pi(\boldsymbol y)  \sum\limits_{i = 1}^Q p_i \pi(\boldsymbol x_i|\boldsymbol y_{-i}) \mathbb{I}(\boldsymbol x_{-i} = \boldsymbol y_{-i})d\mu(\boldsymbol y) \\
&=  \sum\limits_{i = 1}^Q p_i \int\limits_{\mathbb{R}^{l(1)}} \pi(\boldsymbol y) \pi(\boldsymbol x_i|\boldsymbol y_{-i}) \mathbb{I}(\boldsymbol x_{-i} = \boldsymbol y_{-i})d\mu(\boldsymbol y) \\
&=  \sum\limits_{i = 1}^Q p_i \int\limits_{\mathbb{R}^{l(1) - l_i}} \pi(\boldsymbol y_{-i}) \pi(\boldsymbol x_i|\boldsymbol y_{-i}) \mathbb{I}(\boldsymbol x_{-i} = \boldsymbol y_{-i})d\mu(\boldsymbol y_{-i}) \\
&=  \sum\limits_{i = 1}^Q p_i \int\limits_{\mathcal{X}^{l(1) - l_i}} \pi(\boldsymbol x_i, \boldsymbol y_{-i}) \mathbb{I}(\boldsymbol x_{-i} = \boldsymbol y_{-i})d\mu(\boldsymbol y_{-i})\\
&=  \sum\limits_{i = 1}^Q p_i \pi(\boldsymbol x_i, \boldsymbol x_{-i}) \\
&= \pi(\boldsymbol x)
\end{align*}

Thus all three sampling strategies yield the joint density, $\pi(\cdot)$ as their invariant distribution.
\end{proof}


Recall that a Markov chain is \emph{Harris ergodic} if it is $\phi$-irreducible, aperiodic, Harris recurrent, and possesses invariant distribution $\Pi$ for some measures $\phi$ and $\Pi$.

To prove Theorem \ref{thm:gibbs_harris}, we employ the following Lemma based on Johnson (2009).

\begin{lemma}[Johnson, 2009] 
\label{lemma:harris}
Let $P$ denote the one-step transition kernel of a $d$-component Gibbs sampler. Assume $P(\boldsymbol x, \cdot)$ is absolutely continuous with respect to invariant distribution $\pi$. Also, for CGS and RQGS, suppose $P(\boldsymbol x, A) > 0$ for any $x \in \mathcal{X}$ and $A \in \mathcal{F}$ for which $\Pi(A) > 0$. On the other hand, suppose the $d$-step RSGS transition kernel $P^d(\boldsymbol x, A) > 0$. Then the Gibbs sampler is Harris ergodic. The $m$-step transition kernel $P^{(m)}(x, \cdot)$ converges to $\Pi(\cdot)$ in total variation, i.e. as $m \rightarrow \infty$,
$$
\sup\limits_{A \in \mathcal{B}}|P^{(m)}(\boldsymbol x, A) - \Pi(A)|\downarrow 0 \text{ as } m \rightarrow \infty.
$$
\end{lemma}


\begin{proof}[\bf Proof of Theorem \ref{thm:gibbs_harris}]
By Lemma \ref{lemma:invariant}, it suffices to show that the three Gibbs samplers (CGS, RQGS, and RSGS) are Harris ergodic by applying Lemma \ref{lemma:harris}.

Note that all three transition densities, $k_{CGS}(\cdot, \cdot)$, $k_{RQGS}(\cdot, \cdot)$, and $k_{RSGS}(\cdot, \cdot)$ are positive on the support $\mathcal{X} \subset \mathbb{R}^n$ of $\pi(\cdot)$ by Condition \ref{condition:conc-pos}. 

Let $A \in \mathcal{F}$ (where $\mathcal{F}$ is the $\sigma$-algebra associated with $\mathcal{X}$) be such that $\Pi(A) = 0$. Then, by definition $\Pi(A) = \int_A\pi(\boldsymbol x) d\mu(\boldsymbol x) = 0$ where $\mu$ is the dominating measure on $\mathcal{X}$. Now, since the invariant distribution $\pi$ is positive on $A \subset \mathcal{X}$, this implies $\mu(A) = 0$. Therefore $P_{\sigma}(\boldsymbol x, A) = \int_A k_{\sigma}(\boldsymbol y, \boldsymbol x) d\mu(\boldsymbol y) = 0$ where $\sigma \in \{\text{CGS, RQGS, RSGS}\}$. Thus, $P(\boldsymbol x, \cdot)$ is absolutely continuous with respect to invariant distribution $\Pi$ for each of the sampling strategies.

Now, let $A \subset \mathcal{F}$ (i.e. $A \subset \mathcal{X}$) be such that $\Pi(A) > 0$. Then $\Pi(A) = \int_A\pi(\boldsymbol x) d\mu(\boldsymbol x) > 0$, implying $\mu(A) > 0$ must hold by the positivity of $\pi(\cdot)$ on $A$. Since $k_{CGS}(\cdot, \cdot)$, and $k_{RQGS}(\cdot, \cdot)$ are positive on $\mathcal{X}$, this implies $P_{CGS}(\boldsymbol x, A) > 0$ and $P_{RQGS}(\boldsymbol x, A) > 0$ hold for any $\boldsymbol x \in \mathcal{X}$. Finally, for $d = Q$, the $d$-step transition kernel for RSGS is defined as follows: 
\begin{align*}
P^Q_{RSGS}(\boldsymbol x, A) &= P(\boldsymbol X^{(i + Q)} \in A | \boldsymbol X^{(i)} = \boldsymbol x) \\
&= \int\limits_A  k_{RSGS}^Q(\boldsymbol x, \boldsymbol y) d\mu(\boldsymbol y), A \in \mathcal{F},
\end{align*}
where
$$
k_{RSGS}^Q(\boldsymbol x, \boldsymbol y) = \int\limits_{\mathcal{X}^{l(1)}} k_{RSGS}(\boldsymbol x, \boldsymbol z)k_{RSGS}^{(Q-1)}(\boldsymbol z, \boldsymbol y) d\mu(\boldsymbol z)
$$

We will proceed by induction to show $P^k_{RSGS}(\boldsymbol x, A) > 0$ holds for any $A \in \mathcal{F}$ ($A \subset \mathcal{X}$), $\boldsymbol x \in \mathcal{X}$, and $k \ge 1$. For $Q = 1$, pick and fix $A \in \mathcal{F}$, $\boldsymbol x \in \mathcal{X}$. Then for $k = 1$, it holds that
\begin{align*}
P^1_{RSGS}(\boldsymbol x, A) = P(\boldsymbol X^{(i + 1)} \in A | \boldsymbol X^{(i)} = \boldsymbol x) =\int\limits_A  k_{RSGS}(\boldsymbol x, \boldsymbol y) d\mu(\boldsymbol y) > 0
\end{align*}
due to the fact that $k_{RSGS}(\cdot, \cdot)$ is positive on $\mathcal{X}$. Now assume $P^{k-1}_{RSGS}(\boldsymbol x, A) > 0$ holds for some integer $k - 1$ and any $A \in \mathcal{F}$ and $\boldsymbol x \in \mathcal{X}$. Then by the Fubini-Tonelli Theorem and the definition of $k^k_{RSGS}(\boldsymbol x, \boldsymbol y)$ in terms of $k^{k-1}_{RSGS}(\boldsymbol x, \boldsymbol y)$, we have
\begin{align*}
P^k_{RSGS}(\boldsymbol x, A) &=\int\limits_A  k_{RSGS}^k(\boldsymbol x, \boldsymbol y) d\mu(\boldsymbol y) \\
&=\int\limits_A \int\limits_{\mathcal{X}} k_{RSGS}(\boldsymbol x, \boldsymbol z)k_{RSGS}^{(k-1)}(\boldsymbol z, \boldsymbol y) d\mu(\boldsymbol z)d\mu(\boldsymbol y) \\
&= \int\limits_{\mathcal{X}} k_{RSGS}(\boldsymbol x, \boldsymbol z)\int\limits_A k_{RSGS}^{(k-1)}(\boldsymbol z, \boldsymbol y) d\mu(\boldsymbol y) d\mu(\boldsymbol z)\\
&= \int\limits_{\mathcal{X}} k_{RSGS}(\boldsymbol x, \boldsymbol z) P^{k-1}_{RSGS}(\boldsymbol z, A)d\mu(\boldsymbol z) > 0.
\end{align*}
Again, due to the fact that $k_{RSGS}(\cdot, \cdot)$ is positive on $\mathcal{X}$, as well as the induction assumption that $P^{k-1}_{RSGS}(\boldsymbol x, A) > 0$ for some integer any $A \in \mathcal{F}$, $\boldsymbol x \in \mathcal{X}$.

Thus, by Lemma \ref{lemma:harris} all three Gibbs sampling strategies are Harris ergodic.
\end{proof}

\begin{proof}[\bf Proof of Theorem \ref{thm:2}]
Recall that a Markov chain on a space $\mathcal{X}$ is \emph{geometrically ergodic} if there exists some function $G: \mathcal{X} \rightarrow \mathbb{R}$ and some constant $t \in (0,1)$ that satisfy
$$
\Vert P^n(\boldsymbol x, \cdot) - \pi(\cdot) \Vert \le G(\boldsymbol x)t^n \quad \text{ for any } \boldsymbol x \in \mathcal{X}.
$$
We shall use the following Lemma \ref{lemma:geometric} to establish Theorem \ref{thm:2}.

\begin{lemma}[Johnson and Burbank, 2015]
\label{lemma:geometric}
Suppose a 2 component Gibbs sampler is Harris ergodic and for all $(y_{1}, y_{2}), (y_{1n}, y_{2n}) \in Y_1\times Y_2$ such that $(y_{1n}, y_{2n}) \rightarrow (y_1, y_2)$, 
$$
\pi\left(y_2|\liminf\limits_{n \rightarrow \infty} y_{1n} \right) \le \liminf\limits_{n \rightarrow \infty} \pi(y_2|y_{1n}) \text{ and }
\pi\left(y_1|\liminf\limits_{n \rightarrow \infty} y_{2n} \right) \le \liminf\limits_{n \rightarrow \infty} \pi(y_1|y_{2n})
$$
holds, where $\pi(\cdot|\cdot)$ denotes the conditional densities for each of the two components of the sampler. Also suppose that there exist functions $f: Y_1 \rightarrow [1, \infty)$ and $g: Y_2 \rightarrow [1, \infty)$ and constants $j,k,u,v > 0$ such that $ju < 1$ and 
\begin{align}
E[f(Y_1)|y_2] \le jg(y_2) + k \text{ and } E[g(Y_2)|y_1] \le uf(y_1) + v. \label{thm:geometric_eqn1}
\end{align}
If $C_d \equiv \{y_2: g(y_2) \le d\}$ is compact for all $d > 0$, then the two component Gibbs sampler is geometrically ergodic.
\end{lemma}

Under the assumptions, Theorem \ref{thm:gibbs_harris} yields that all three Gibbs samplers are Harris ergodic with stationary distribution given by the full joint. We next apply Lemma \ref{lemma:geometric} noting that we are assuming that $Q = 2$ concliques are available under the four-nearest neighborhood structure for the MRF model with lattice data. That is, in the notation of Lemma \ref{lemma:geometric}, we have a two component Gibbs sampler for $(Y_1, Y_2)$ with components $Y_1 =\{Y(\boldsymbol s_i):\boldsymbol s_i \in \mathcal{C}_1\}$ and $Y_2 =\{Y(\boldsymbol s_i):\boldsymbol s_i \in \mathcal{C}_2\}$ defined by dividing observations $(Y(\boldsymbol s_1), \dots, Y(\boldsymbol s_n)$ into $Q = 2$ concliques. By Theorem \ref{thm:2} assumptions, the full conditionals $f_i(y(\boldsymbol s_i)|\boldsymbol y(\mathcal{N}_i))$ from (\ref{eqn:1}) are continuous in $\boldsymbol y(\mathcal{N}_i)$. Suppose the locations in conclique 1 may be written as $s_{i_1}, \dots, s_{i_\ell}$ for $\{i_1, \dots, i_\ell\} \subset \{1, \dots, n\}$ and $1 \le \ell \le n$. Then the transition density $\pi(y_1|y_2)$ of $Y_1$ (conclique 1 values) given $Y_2$ (conclique 2 values) may be written as $\pi(y_1|y_2) = \prod\limits_{j = 1}^\ell f_{i_j}(y(\boldsymbol s_{i_j})|\boldsymbol y(\mathcal{N}_{i_j}))$ where by the Markov property, $f_{i_j}(y(\boldsymbol s_{i_j})|Y_2) = f_{i_j}(y(\boldsymbol s_{i_j})|\boldsymbol y(\mathcal{N}_{i_j}))$ holds as $\boldsymbol y(\mathcal{N}_{i_j}) \subset Y_2$ for $j = 1, \dots, \ell$. Since each full conditional density $f_{i_j}(y(\boldsymbol s_{i_j})|\boldsymbol y(\mathcal{N}_{i_j})) = f_{i_j}(y(\boldsymbol s_{i_j})|Y_2)$ is continuous in $y_2$, the transition density $\pi(y_1|y_2)$ is continuous in $y_2$ so that if $(y_{1n}, y_{2n}) \rightarrow (y_1, y_2)$, then $\pi\left(y_1|\liminf\limits_{n \rightarrow \infty} y_{2n} \right) = \pi(y_1|y_2) = \liminf\limits_{n \rightarrow \infty} \pi(y_1|y_{2n})$ holds. The same argument holds upon switching the conditioning roles of $Y_1$ and $Y_2$ (conclique 1 and conclique 2). Thus, by Lemma \ref{lemma:geometric}, Theorem \ref{thm:2} will follow by establishing (\ref{thm:geometric_eqn1}) holds with observations $Y_1$ and $Y_2$ from conclique 1 and 2, respectively.

To this end, define $f(\boldsymbol y_1) = 1$ and $g(\boldsymbol y_2) = 1$ for $y_1 \in \mathcal{X}_1$ and $y_2 \in \mathcal{X}_2$ where $\mathcal{X}_i$ is the support of observations in $\mathcal{C}_i$ for $ = 1,2$. Without loss of generality, suppose $\mathcal{X}_2$ is compact under Theorem 2 assumptions. Then it holds that
$$
E(f(\boldsymbol Y_1)|\boldsymbol y_2) = 1  \le j + 1 = jg(\boldsymbol y_2) + k \quad \text{and} \quad E(g(\boldsymbol Y_2)|\boldsymbol y_1) = 1  \le u + 1 = uf(\boldsymbol y_1) + v.
$$

for any constants $j,u > 0$ such that $ju < 1$ with $k = 1$ and $v = 1$. This verifies (\ref{thm:geometric_eqn1}). Finally, let $d > 0$. Then $C_d = \{\boldsymbol y_2 \in \mathcal{X}_2 : g(\boldsymbol y_2) \le d\} \subset  \mathcal{X}_2$, where the latter set is compact. Thus, $C_d$ is compact and Lemma \ref{lemma:geometric} holds for any two component conclique based Gibbs sampler among CGS, RQGS, and RSGS. Therefore, CGS, RQGS, and RSGS samplers are geometrically ergodic.
\end{proof}

\begin{proof}[\bf Proof of Theorem \ref{thm:cases}]
~\\
\emph{Gaussian: } \\
Let $Y(\boldsymbol s_i)$ be conditionally Gaussian distributed given $\boldsymbol Y(\mathcal{N}_i)$ using a four-nearest neighbor structure and having expected values $\{\mu(\boldsymbol s_i): i = 1, \dots, n\}$ and constant conditional variance $\tau^2$ where 
$$
\mu(\boldsymbol s_i) = \alpha + \eta\sum\limits_{s_j \in \mathcal{N}_i}\{y(s_j) - \alpha\}.
$$
Then, this model is of the form specified in (\ref{eqn:exp_fam}) with
$$
A_{1i}(\boldsymbol y(\mathcal{N}_i)) = \frac{1}{2\tau^2}\mu(\boldsymbol s_i), \qquad T_1(y(\boldsymbol s_i)) = y(\boldsymbol s_i), \qquad B_i(\boldsymbol y(\mathcal{N}_i)) = -2\tau^2 A_{1i}(\boldsymbol y(\mathcal{N}_i))^2
$$

This model specifies a valid joint distribution for $|\eta| < 0.25$ (Cressie 1993) and from Theorem \ref{thm:gibbs_harris} we have that the conclique-based Gibbs strategies yield a Harris ergodic sampler for this model (as there are 2 concliques). Since the support for $Y(\boldsymbol s_i) | \boldsymbol Y(\mathcal{N}_i)$ is not compact, we cannot use Theorem \ref{thm:2} to show geometric ergodicity. However by the proof of Theorem \ref{thm:2}, it suffices to establish (\ref{thm:geometric_eqn1}) and the compactness of $C_d = \{y_2 \in g(y_2) \le d\}$ for $d >0$ where, as in the proof of Theorem \ref{thm:2}, $Y_1 \in \mathcal{X}_1$ and $Y_2 \in \mathcal{X}_2$ denote the observations in conclique $\mathcal{C}_1$ and $\mathcal{C}_2$, respectively, and $f(\cdot)$ and $g(\cdot)$ denote functions of $Y_1$ and $Y_2$ in (\ref{thm:geometric_eqn1}). Here $\mathcal{X}_i = \mathbb{R}^{m_i}$ holds where $m_i$ denotes the number of observations (locations) in $\mathcal{C}_i, i = 1, 2$.

Define 
$$
f(\boldsymbol y_1) = \langle \boldsymbol y_1 - a, \boldsymbol y_1 - a \rangle + 1, y_1 \in \mathbb{R}^{m_1} \quad \text{and} \quad g(\boldsymbol y_2) = \langle \boldsymbol y_2 - b, \boldsymbol y_2 - b \rangle + 1, y_2 \in \mathbb{R}^{m_2} \\
$$
where $\langle \cdot, \cdot \rangle$ denotes the vector dot product and 
$$
a = \frac{1}{4}\frac{(1 + 4\eta) \alpha \eta}{(1 + 4\eta)\alpha - \eta^2} 
\quad \text{and} \quad 
b = \frac{1}{4}\frac{(1 + 4\eta)\alpha \eta}{(1 + 4\eta)\alpha - \eta^2}.
$$

Now, let $c$ denote a generic constant that does not depend on $\boldsymbol y_2$ and define $\boldsymbol \mu_1 = \text{E}(Y_1|Y_2)$ as a vector of conditional means under the Gaussian model corresponding to the locations in $\mathcal{C}_1$. In what follows, without loss of generality, we assume the $m_1$ locations in conclique $\mathcal{C}_1$ are $\mathcal{C}_1 = \{s_1, \dots, s_{m_1}\}$ for simplicity. Then for $\boldsymbol y_2 \in \mathbb{R}^{m_2}$, we have
\begin{align*}
E(f(\boldsymbol Y_1)|\boldsymbol y_2) &= E(\langle \boldsymbol Y_1 - a, \boldsymbol Y_1 - a \rangle + 1|\boldsymbol y_2) \\
&= E(\langle \boldsymbol Y_1, \boldsymbol Y_1 \rangle - 2\langle \boldsymbol Y_1, a \rangle + m_1a^2 + 1 |\boldsymbol y_2) \\
&= \langle \boldsymbol \mu_1, \boldsymbol \mu_1 \rangle + m_1\tau^2 - 2\langle \boldsymbol \mu_1, a \rangle + m_1 a^2 + 1 \\
&= \sum\limits_{i = 1}^{m_1}\left(\alpha + \eta\sum\limits_{s_j \in \mathcal{N}_i}(y(s_j) - \alpha)\right)^2 + m_1\tau^2 - 2a \sum\limits_{i = 1}^{m_1}\left(\alpha + \eta\sum\limits_{s_j \in \mathcal{N}_i}(y(s_j) - \alpha)\right) + m_1a^2 + 1 \\
&= m_1((1 + 4\eta)^2\alpha^2 + a^2 + \tau^2) + 2(1 + 4\eta)\alpha(\eta - a)4\sum\limits_{i = 1}^{m_2} y_{2i} + \eta^2\sum\limits_{i = 1}^{m_1}\left(\sum\limits_{s_j \in \mathcal{N}_i} y(s_j)\right)^2 \\
&\le m_1((1 + 4\eta)^2\alpha^2 + a^2 + \tau^2) + 2(1 + 4\eta)\alpha(\eta - a)4\sum\limits_{i = 1}^{m_2} y_{2i} + 4\eta^2\sum\limits_{i = 1}^{m_1}\sum\limits_{j = 1}^4 y(s_{ij})^2\\
&\le m_1((1 + 4\eta)^2\alpha^2 + a^2 + \tau^2) + 2(1 + 4\eta)\alpha(\eta - a)4\sum\limits_{i = 1}^{m_2} y_{2i} + 16\eta^2\sum\limits_{i = 1}^{m_2} y_{2i}^2\\
&= j \sum\limits_{i = 1}^{m_2} y_{2i}^2 + 2j\frac{(1 + 4\eta)\alpha(\eta - a)}{4\eta^2}\sum\limits_{i = 1}^{m_2} y_{2i} + c \\
&= j \sum\limits_{i = 1}^{m_2} y_{2i}^2 - 2j b\sum\limits_{i = 1}^{m_2} y_{2i} + c \\
&= jg(\boldsymbol y_2) + k. 
\end{align*}
for some $k > 0$, where the inequality above follows from Jensen's inequality and $j = 16\eta^2$ above. Similarly, 
\begin{align*}
E(g(\boldsymbol Y_2)|\boldsymbol y_1) 
&\le uf(\boldsymbol y_1) + v
\end{align*}
holds with $u = 12\eta^2$ and some $v > 0$. Thus, (\ref{thm:geometric_eqn1}) of Lemma \ref{lemma:geometric} is satisfied where $ju = 16^2\eta^4 < 1$ by the model assumption of $|\eta| < 0.25$. 

Now let $d > 0$ and note that

\begin{align*}
C_d &= \{\boldsymbol y_2 \in \mathbb{R}^{m_2} : g(\boldsymbol y_2) \le d\} = \{\langle \boldsymbol y_2 - b, \boldsymbol y_2 - b \rangle + 1 \le d\} =\{\Vert\boldsymbol y_2 - b\Vert^2  \le d - 1\}
\end{align*}

is compact because $C_d$ is empty for $0 < d < 1$ and a closed ball of radius $d-1$ when $d \ge 1$. Thus, Lemma \ref{lemma:geometric} applies and the CGS, RQGS, and RSGS samplers are geometrically ergodic for the Gaussian case.

\emph{(Centered) Inverse Gaussian: } \\
Let $Y(\boldsymbol s_i)$ be conditionally Inverse Gaussian (IG) distributed given $\boldsymbol Y(\mathcal{N}_i)$ using a four-nearest neighbor structure with a density 
\begin{align*}
f_i(y(\boldsymbol s_i) | \boldsymbol y(\mathcal{N}_1), \mu, \lambda, \eta_1, \eta_2) &= \exp \left\{\frac{A_{1i}(\boldsymbol y(\mathcal{N}_i))}{2} y_i - \frac{A_{2i}(\boldsymbol y(\mathcal{N}_i))}{2} \frac{1}{y_i} -B_i(\boldsymbol y(\mathcal{N}_i)) + C(y(\boldsymbol s_i)) \right\}
\end{align*}
where 
$$
A_{1i}(\boldsymbol y(\mathcal{N}_i)) = \alpha_1 + \eta_1 \sum\limits_{\boldsymbol s_j}\left(\frac{1}{y(\boldsymbol s_j)} - \frac{1}{\mu} - \frac{1}{\lambda}\right) 
\quad \text{and} \quad 
A_{2i}(\boldsymbol y(\mathcal{N}_i)) = \alpha_2 + \eta_2 \sum\limits_{\boldsymbol s_j}\left(y(\boldsymbol s_j) - \mu \right),
$$
for $\mu, \lambda > 0, \eta_1, \eta_2 \ge 0$,  and $\alpha_1  = \lambda/\mu^2 >0,  \alpha_2 = \lambda > 0$. 

For this model, the conditional mean of $Y_i$ is $\sqrt{A_{2i}(\boldsymbol y(\mathcal{N}_i))/A_{1i}(\boldsymbol y(\mathcal{N}_i))}$ and the conditional mean of $1/Y_i$ is $\sqrt{A_{1i}(\boldsymbol y(\mathcal{N}_i))/A_{2i}(\boldsymbol y(\mathcal{N}_i))} + 1/A_{2i}(\boldsymbol y(\mathcal{N}_i))$. In order for this model to be valid (i.e. $A_{1i}(\boldsymbol y(\mathcal{N}_i)), A_{2i}(\boldsymbol y(\mathcal{N}_i)) \ge 0$), we need $\lambda, \mu > 0$ with $\eta_1, \eta_2 \ge 0$, or equivalently
\begin{align*}
\alpha_1 - 4\eta_1\left(\frac{1}{\mu} + \frac{1}{\lambda}\right) > 0 \qquad &\text{ and } \qquad \alpha_2 - 4\eta_2  \mu > 0, \\
\text{or} \quad 0 \le \eta_1 \le \frac{\lambda^2}{4\mu(\lambda + \mu)} \qquad &\text{ and } \qquad 0 \le \eta_2 \le \frac{\lambda}{4\mu}.
\end{align*}
in the four-nearest neighborhood structure.

For technical reasons related to geometric ergodicity, we extend and close the IG model support from $(0, \infty)$ to $[0, \infty)$ without changing the joint distribution of $(Y(\boldsymbol s_1), \dots, Y(\boldsymbol s_n))$. To accomplish this, 
\begin{enumerate}
\item We declare the conditional distribution for $Y(\boldsymbol s_i) | \{Y(\boldsymbol s_j): \boldsymbol s_j \in \mathcal{N}_i \}$ to be $IG(1,1)$ if any conditioning variables are zero among $\{Y(\boldsymbol s_j): \boldsymbol s_j \in \mathcal{N}_i \}$, and
\item extend the density of any IG distribution to be $\infty$ when the argument is zero, i.e. $f_i(y|\cdot) = \infty$ at $y = 0$.
\end{enumerate}

Let $m_i$ be the number of locations in $\mathcal{C}_i$ for $i = 1,2$. Now, we can write
\begin{align*}
A_{1i}(\boldsymbol y(\mathcal{N}_i)) = \tilde{\alpha}_1 + \eta_1 \sum\limits_{\boldsymbol s_j \in \mathcal{N}_i} \frac{1}{y(\boldsymbol s_j)}, &\qquad \tilde{\alpha}_1 = \alpha_1 - 4\eta_1\left(\frac{1}{\mu} + \frac{1}{\lambda}\right) > 0, \\
A_{2i}(\boldsymbol y(\mathcal{N}_i)) = \tilde{\alpha}_2 + \eta_2 \sum\limits_{\boldsymbol s_j \in \mathcal{N}_i} y(\boldsymbol s_j), &\qquad \tilde{\alpha}_2 = \alpha_2 - 4\eta_2\mu > 0,
\end{align*}
assuming that none of $y(\boldsymbol s_j), \boldsymbol s_j \in \mathcal{N}_i$ are zero.

By the structure of the IG conditional densities, it suffices to establish geometric ergodicity of the CGS, RSGS, and RQGS samplers by verifying (\ref{thm:geometric_eqn1}) in an application of Lemma \ref{lemma:geometric} with $Y_1 = \{Y(\boldsymbol s_i): \boldsymbol s_i \in \mathcal C_1\} \equiv (Y_{11}, \dots, Y_{1m_1})$ and $Y_2 = \{Y(\boldsymbol s_i): \boldsymbol s_i \in \mathcal C_2\} \equiv (Y_{12}, \dots, Y_{1m_2})$.

Now define 
$$
f(\boldsymbol y_1) = \sum\limits_{i=1}^{m_1}\boldsymbol y_{1i} + 1, \boldsymbol y_1 \in [0, \infty)^{m_1} \quad \text{and} \quad g(\boldsymbol y_2) = \sum\limits_{i=1}^{m_2}\boldsymbol y_{2i} + 1, \boldsymbol y_2 \in [0, \infty)^{m_2}.
$$

In the following, write $Y(\boldsymbol s_i) = Y_i, i = 1, \dots, n$ for simplicity. Then for $\boldsymbol y_2 \in [0, \infty)^{m_2}$, letting $\mathbb{I}(\cdot)$ denote the indicator function and defining $\theta_1 = 4(\eta_2/\tilde{\alpha}_1)^{1/2}$, we have
\begin{eqnarray*}
&E(f(\boldsymbol Y_1)|\boldsymbol y_2)&= \sum\limits_{\{1 \le i \le m_1:\text{some } Y_j = 0 for \boldsymbol s_j \in \mathcal{N}_1\}} E(IG(1,1)) + \sum\limits_{\{1 \le i \le m_1:\text{some } Y_j \not= 0 for \boldsymbol s_j \in \mathcal{N}_1\}}E(Y_{1j}|\boldsymbol y_2) + 1\\
&=&  \sum\limits_{\{1 \le i \le m_1:\text{some } Y_j = 0 for \boldsymbol s_j \in \mathcal{N}_1\}} 1 + \sum\limits_{\{1 \le i \le m_1:\text{some } Y_j \not= 0 for \boldsymbol s_j \in \mathcal{N}_1\}} \sqrt{\frac{A_{2i}(\boldsymbol y(\mathcal{N}_i))}{A_{1i}(\boldsymbol y(\mathcal{N}_i))}} + 1\\
&\le& m_1 + \sum\limits_{\{1 \le i \le m_1:\text{some } Y_j \not= 0 for \boldsymbol s_j \in \mathcal{N}_1\}}\frac{(\tilde{\alpha_2})^{1/2} + \left(\eta_2\sum\limits_{\boldsymbol j \in \mathcal{N}_i}y_j\right)^{1/2}}{(\tilde{\alpha}_1 + \eta_2\sum\limits_{\boldsymbol j \in \mathcal{N}_i}1/y_j)^{1/2}} + 1\\
&\le& m_1 + \sum\limits_{\{1 \le i \le m_1:\text{some } Y_j \not= 0 for \boldsymbol s_j \in \mathcal{N}_1\}}\frac{(\tilde{\alpha_2})^{1/2} + \left(\eta_2\sum\limits_{\boldsymbol j \in \mathcal{N}_i}y_j\right)^{1/2}}{(\tilde{\alpha}_1)^{1/2}} + 1\\
&\le& m_1\left(1 + \left(\frac{\tilde{\alpha}_2}{\tilde{\alpha_1}}\right)^{1/2}\right) + \sum\limits_{i = 1}^{m_1}\left(\frac{\eta_2}{\tilde{\alpha}_1}\right)^{1/2} \left(\sum\limits_{\boldsymbol j \in \mathcal{N}_i}y_j\right)^{1/2} + 1 \\
&\le& m_1\left(1 + \left(\frac{\tilde{\alpha}_2}{\tilde{\alpha_1}}\right)^{1/2}\right) + 1 + \frac{1}{4}\theta_1\sum\limits_{i = 1}^{m_1} \left(\sum\limits_{\boldsymbol j \in \mathcal{N}_i}y_j\right)^{1/2}\left[\mathbb{I}\left(\left(\sum\limits_{\boldsymbol j \in \mathcal{N}_i}y_j\right)^{1/2} \le 2\theta_1\right) + \mathbb{I}\left(\left(\sum\limits_{\boldsymbol j \in \mathcal{N}_i}y_j\right)^{1/2} > 2\theta_1\right)\right] \\
&\le&  m_1\left(1 + \left(\frac{\tilde{\alpha}_2}{\tilde{\alpha_1}}\right)^{1/2} + \frac{1}{2} \theta_1^2\right) + 1 + \frac{1}{4}\frac{1}{2}\sum\limits_{i = 1}^{m_1} \sum\limits_{\boldsymbol j \in \mathcal{N}_i}y_j \\
&\le& c + \frac{1}{2}g(\boldsymbol y_2) \\
&=& jg(\boldsymbol y_2) + k. 
\end{eqnarray*}
for $j = \frac{1}{2}$ and some $k > 0$. Similarly, it holds that
\begin{align*}
E(g(\boldsymbol Y_2)|\boldsymbol y_1) 
&\le uf(\boldsymbol y_1) + v
\end{align*}
for $u = \frac{1}{2}$ and some $v > 0$. Thus, (\ref{thm:geometric_eqn1}) of Lemma \ref{lemma:geometric} is satisfied with $ju = \frac{1}{4} < 1$.

And for $d > 0$, we have

\begin{align*}
C_d &= \{\boldsymbol y_2 \in [0, \infty)^{m_2} : g(\boldsymbol y_2) \le d\} = \left\{\boldsymbol y_2 \in [0, \infty)^{m_2} :\sum\limits_{i = 1}^{m_2} y_{2i} + 1 \le d\right\}
\end{align*}
which is compact as $C_d = \emptyset$ if $0 < d \le 1$ and for $d > 1$, $C_d$ is a closed ball of radius $d - 1$ under the $L_1$ norm. Thus, Lemma \ref{lemma:geometric} holds.  So it follows that the CGS, RQGS, and RSGS are geometrically ergodic for the IG case.

\emph{(Centered) Truncated Gamma: } \\
Let $Y(\boldsymbol s_i)$ be conditionally Gamma distributed (truncated such that $Y(\boldsymbol s_i) \ge 1$), given $\boldsymbol Y(\mathcal{N}_i)$ using a four-nearest neighbor structure where 
\begin{align*}
f_i(y(\boldsymbol s_i) | \boldsymbol \theta) &= \exp \left\{A_{1i}(\boldsymbol y(\mathcal{N}_i)) \log(y_i) - A_{2i}(\boldsymbol y(\mathcal{N}_i)) y_i -B_i(\boldsymbol y(\mathcal{N}_i))) \right\}, \quad y(\boldsymbol s_i) \ge 1,
\end{align*}
where 
$$
A_{1i}(\boldsymbol y(\mathcal{N}_i)) = \alpha_1 + \eta \sum\limits_{\boldsymbol s_j \in \mathcal{N}_i}\log(y(\boldsymbol s_j)) \quad \text{and} \quad A_{2i}(\boldsymbol y(\mathcal{N}_i)) = \alpha_2 \\
$$
for $\eta > 0, \alpha_1 >-1,  \alpha_2 > 0$. That is $Y(\boldsymbol s_i)|\boldsymbol y(\mathcal{N}_i)$ is a gamma with scale and shape parameters $A_{1i}(\boldsymbol y(\mathcal{N}_i)) + 1$ and $1/A_{2i}(\boldsymbol y(\mathcal{N}_i))$.

This model specifies a valid joint distribution, so from Theorem \ref{thm:gibbs_harris} we have that the conclique-based Gibbs strategies yield a Harris ergodic sampler for this model. Again, by the structure of the truncated Gamma conditionals, it suffices to establish geometric ergodicity of the CGS (and RSGS/RQGS) by verifying (\ref{thm:geometric_eqn1}) in an application of Lemma \ref{lemma:geometric} with $Y_1 = \{Y(\boldsymbol s_i): \boldsymbol s_i \in \mathcal C_1\} \equiv (Y_{11}, \dots, Y_{1m_1})$ and $Y_2 = \{Y(\boldsymbol s_i): \boldsymbol s_i \in \mathcal C_2\} \equiv (Y_{12}, \dots, Y_{1m_2})$ where $m_k$ denotes the number of observations/locations in conclique $\mathcal{C}_k, k = 1,2$.

Define functions of conclique observations as 
$$
f(\boldsymbol y_1) = \sum\limits_{i = 1}^{m_1} y_{1i} \quad \text{and} \quad g(\boldsymbol y_2) = \sum\limits_{i = 1}^{m_2} y_{2i}. \\
$$
Let $c$ and $\tilde{c}$ denote generic constants that do not depend on $\boldsymbol y_2$ and let $\mathbb{I}(\cdot)$ denote the indicator function. Then, for $\boldsymbol y_2 \in [1, \infty)^{m_2}$, noting that the conditional truncated gamma distribution of $Y(\boldsymbol s_i)$ has a mean bounded by that of a gamma variable with scale $A_{1i}(\boldsymbol y(\mathcal{N}_i))$ and shape $1/A_{2i}(\boldsymbol y(\mathcal{N}_i))$ parameters, we have that
\begin{align*}
E(f(\boldsymbol Y_1)|\boldsymbol y_2) &= E\left(\sum\limits_{i = 1}^{m_1} Y_{1i} |\boldsymbol y_2\right) \\
&= \sum\limits_{i = 1}^{m_1} E(Y_{1i} |\boldsymbol y_2) \\
&\le \sum\limits_{i = 1}^{m_1} \frac{1}{\alpha_2}\left(\alpha_1 + \eta \sum_{j \in \mathcal{N}_i} \log(y(\boldsymbol s_j)) + 1 \right) \\
&= c + \frac{4\eta}{\alpha_2}\sum\limits_{j = 1}^{m_2} \log(y_{2j}) \\
& \le c + \frac{4\eta}{\alpha_2}\sum\limits_{j = 1}^{m_2} \sqrt{y_{2j}} \\
& = c + \frac{4\eta}{\alpha_2}\sum\limits_{j = 1}^{m_2} \left[\sqrt{y_{2j}}\mathbb{I}\left(\sqrt{y_{2j}} \le 2\frac{4\eta}{\alpha_2}\right) + \sqrt{y_{2j}}\mathbb{I}\left(\sqrt{y_{2j}} > 2\frac{4\eta}{\alpha_2}\right)\right] \\
& \le c + \frac{4\eta}{\alpha_2}\sum\limits_{j = 1}^{m_2} \left[ 2\frac{4\eta}{\alpha_2} + \frac{y_{2j}}{2*4\eta/\alpha_2}\right]\\
& = \tilde{c} + \frac{1}{2}\sum\limits_{j = 1}^{m_2} y_{2j}\\
&= jg(\boldsymbol y_2) + k,
\end{align*}
for $j = \frac{1}{2}$ and some $k > 0$, using above that $\log(y) \le \sqrt{y}$ for $y \ge 1$.
Similarly, 
\begin{align*}
E(g(\boldsymbol Y_2)|\boldsymbol y_1) 
&\le uf(\boldsymbol y_1) + v
\end{align*}
holds for for $u = \frac{1}{2}$ and some $v > 0$. Thus, (\ref{thm:geometric_eqn1}) of Lemma \ref{lemma:geometric} is satisfied where $ju = \frac{1}{4} < 1$. Finally, for $d > 0$,
\begin{align*}
C_d &= \{\boldsymbol y_2 \in [1, \infty)^{m_2} : g(\boldsymbol y_2) \le d\} = \left\{\boldsymbol y_2 \in [1, \infty)^{m_2} :\sum\limits_{i = 1}^{m_2} y_{2i} \le d\right\} 
\end{align*}
is compact ($C_d = \emptyset$ if $0 < d< m_2$ and $C_d$ is closed and bounded for $d \ge m_2$). Thus, Lemma \ref{lemma:geometric} holds. Ergo, the CGS, RQGS, and RSGS are geometrically ergodic for the Truncated Gamma case.
\end{proof}
